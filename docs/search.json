[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini-Project #01: NYC Payroll Analysis",
    "section": "",
    "text": "Executive Summary:\nThe following analysis utilizes the NYC payroll data to examine payroll trends, identify key figures within the dataset, and evaluate three potential policy proposals:\n\nCapping salaries at the mayoral level.\nIncreasing staffing to reduce overtime expenses.\nImplementing a performance-based pay structure.\n\nThe goal is to understand potential savings, key individuals, agencies, and trends in the city’s payroll system. Each policy is analyzed in terms of its financial impact, implementation feasibility, and potential issues on workforce moral and performance.\n\n\n\nIntroduction:\nThe City of New York spends significant amounts on payroll across various departments, with substantial portions allocated to overtime pay and high-level salaries. In an effort to address growing payroll expenses, three potential policy initiatives have been explored in this report. These policies are intended to reduce costs, increase efficiency, and ensure fairness within the payroll structure, all while maintaining employee motivation and performance.\n\n\n\nData Collection and Preparation:\nThe dataset used for this analysis derives from the NYC payroll system. This data is collected in batches using the NYC’s open data platform, where:\n\nFile Formate: The data is in CVS format, reperesenting city payroll records, including employee names, positions, salaries and compensations, and other related details.\nData Processing: Various libraries (e.g., dplyr, jsonlite, httr2, readr, stringr, DT, and scales) are used to clean and process the data.\n\nKey insights drawn from the data include:\n\nEmployee Compensation: Calculations of total compensation for key individuals, including salaries, ovetime pay, and other compensations.\nAggregated Salaries: Analysis of city-wide payroll figures, employee counts, and overtime statistics.\n\n\n\n\nKey Insights from NYC Payroll Data:\n\nTop Paid Individual: The highest city total payroll (regular salary + overtime pay) for a single individual is identified.\nOvertime Trends: The individual with the most overtime hours and the agency with the highest overtime usage are highlighted.\nHighest Paying Agencies: The agency with the highest average total payroll per employee is identified.\nPayroll Growth: Over the past 10 years, the city’s aggregate payroll has grown, with the trend showcasing the need for cost control measure.\n\n\n\n\nPolicy Proposals:\n\nPolicy 1: Capping Salaries at the Mayoral Level\n\nObjective: To limit compensation of city employees to the mayor’s total pay, as a mean to reduce government spending.\nAnalysis:\n\nThe mayor’s salary is calculated by summing his regular pay and overtime for each fiscal year.\nEmployees earning above the mayor’s salary are identified, and their salary reductios are computed by deducting the mayor’s salary from their total pay.\nA benchmark of $10 million in savings was administrated to determine whether the proposal would have significant impact.\nPotential Savings: The total savings from implementing this policy is approximately $3.4 billion. Agencies, such as Department of Pedagogical, Fire Department, and Police Department would bear the brunt of the salary caps.\n\nRecommendation: While the policy could yield total savings exceeding $10 million, it may affect employee morale (e.g., lack of motivation), especially amonst high-level positions. Consideration should be given to alternative savings strategies that target overtime reduction.\n\n\n\nPolicy 2: Increasing Staffing to Reduce Overtime Expenses\n\nObjectives: To reduce the need for overtime by increasing the number of full-time employees in key agencies.\nAnalysis:\n\nOvertime data is analyzed across various agencies, with an estimate of the number of full-time employees needed to replace overtime.\nCost Comparison: The cost of overtime is compared to the potential cost of hiring full-time employees to replace overtime hours.\nSavings Potential: Agencies with the higher overtime costs could save significant amounts by replacing overtime with regular employees.\n\nRecommendation: This policy has the potential for substantial savings, especial in agencies with significant overtime costs. The financial viability of this approach should be further explored with specific focus on high overtime usage agencies.\n\n\n\nPolicy 3: Implementing a Performance-Based Pay Structure\n\nObjective: Introduce a performance-based pay structure where employees are rewarded based on their performance, including task completion, complaints received, and response times.\nAnalysis:\n\nA sample employee dataset is used to create performance-based pay by introducing bonuses for task completion and penalties for complaints. In this case, a 10% bonus is introduced for exceeding 100 tasks and a 5% reduction penalty is calculated for 3+ complaints. The performance is calculated by taking the difference of the bonus and the penalty.\nThe total cost of payroll before and after implementing the performance-based pay structure is compared.\nPotential Savings: By adjusting compensation based on performance, savings are realized in the form of reduced pay for low-performing employees. A total savings of $1.2 million is approximating amongst the Health, the Police, and the Public Works departments.\nVisualization: A boxplot is provided to visualize how compensation would change across agencies under the new pay structure.\n\n\n\n\nRecommendation: This policy could lead to some cost savings, particularly for underperforming employees. However, careful consideration of performance metrics and employee buy-in is needed to ensure effectiveness and fairness in the system.\n\n\n\n\n\nSummary of Findings and Recommendations\n\nCapping Salaries: Potential savings from capping salaries at the mayoral level could be substantial. However, the policy risks significant morale impacts among higher-paid employees. A balance between cost-saving and employee satisfaction should be considered.\nIncreasing Staffing: This proposal could reduce overtime-related expenses significantly, especially for agencies with the highest overtime usage. Further analysis is needed on the recruitment and training costs associated with increasing staffing levels.\nPerformance-Based Pay: Implementing a performance-based pay structure could encourage higher productivity but requires careful planning to ensure fairness and avoid unintended consequences, such as employee dissatisfaction.\n\n\n\n\nConclusion:\nAll three policy proposals offer potential solutions to manage NYC’s growing payroll expenses. However, each comes with its own set of challenges and considerations. A detailed cost-benefit analysis, alongside employee feedback, will be crucial in selecting the most viable strategy. Further testing and pilot programs are recommended before widespread implementation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chenbin Wu",
    "section": "",
    "text": "Hi, I am Chenbin Wu, currently a Graduate student at Baruch College studying Business Analytics and concentrating in Data Analytics.\nHere is a link to my LinkedIn.jhiu\n\nLast Updated: Thursday 03 06, 2025 at 00:29AM"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "",
    "text": "Press Release: ‘Greenest’ Transit Agency of the Year Award\n\nExecutive Summary:\nNew York, NY – March 26, 2025 – In efforts to recognize transit agencies demonstrating exceptional efficiency and sustainability, Green Transit Alliance for Investigation of Variance (GTA IV) is proud to announce the winners of the ‘Greenest’ Transit Agencies awards. These awards are based on an in-dept analysis of public transit data, evaluating agencies across key performance metrics including fuel efficiency, emissions reduction, and operational effectiveness.\n\n\n\n\nAppendix: Analysis and Methodology\n\nIntroduction: \nThis appendix outlines the methodology and metrics used to identify the ‘Greenest’ Transit Agency of the Year Award winners. The selection process involves a detailed review of several key factors such as fuel consumption, emissions per mile, and total operational emissions across various transit systems.\n\n\nData Collection and Analysis:\nData was collected from the following sources:\n- Agency reports and annual performance reviews.\n- Publicly available transit data sets, including the National Transit Database.\n- Customer satisfaction surveys and feedback from various transit agencies.\nData was cleaned and processed for analysis.\n\n\nShow the code\nensure_package &lt;- function(pkg){\n  pkg &lt;- as.character(substitute(pkg))\n  options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n  if(!require(pkg, character.only=TRUE)) install.packages(pkg)\n  stopifnot(require(pkg, character.only=TRUE))\n}\n\nensure_package(dplyr)\nensure_package(httr2)\nensure_package(rvest)\nensure_package(datasets)\nensure_package(purrr)\nensure_package(DT)\nensure_package(stringr)\n\n\nget_eia_sep &lt;- function(state, abbr){\n  state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n  \n  dir_name &lt;- file.path(\"data\", \"mp02\")\n  file_name &lt;- file.path(dir_name, state_formatted)\n  \n  dir.create(dir_name, showWarnings=FALSE, recursive=TRUE)\n  \n  if(!file.exists(file_name)){\n    BASE_URL &lt;- \"https://www.eia.gov\"\n    REQUEST &lt;- request(BASE_URL) |&gt; \n      req_url_path(\"electricity\", \"state\", state_formatted)\n    \n    RESPONSE &lt;- req_perform(REQUEST)\n    \n    resp_check_status(RESPONSE)\n    \n    writeLines(resp_body_string(RESPONSE), file_name)\n  }\n  \n  TABLE &lt;- read_html(file_name) |&gt; \n    html_element(\"table\") |&gt; \n    html_table() |&gt;\n    mutate(Item = str_to_lower(Item))\n  \n  if(\"U.S. rank\" %in% colnames(TABLE)){\n    TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n  }\n  \n  CO2_MWh &lt;- TABLE |&gt; \n    filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt;\n    pull(Value) |&gt; \n    str_replace_all(\",\", \"\") |&gt;\n    as.numeric()\n  \n  PRIMARY &lt;- TABLE |&gt; \n    filter(Item == \"primary energy source\") |&gt; \n    pull(Rank)\n  \n  RATE &lt;- TABLE |&gt;\n    filter(Item == \"average retail price (cents/kwh)\") |&gt;\n    pull(Value) |&gt;\n    as.numeric()\n  \n  GENERATION_MWh &lt;- TABLE |&gt;\n    filter(Item == \"net generation (megawatthours)\") |&gt;\n    pull(Value) |&gt;\n    str_replace_all(\",\", \"\") |&gt;\n    as.numeric()\n  \n  data.frame(CO2_MWh               = CO2_MWh, \n             primary_source        = PRIMARY,\n             electricity_price_MWh = RATE * 10, # / 100 cents to dollars &\n             # * 1000 kWh to MWH \n             generation_MWh        = GENERATION_MWh, \n             state                 = state, \n             abbreviation          = abbr\n  )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\n\nData was collected using the U.S. Energy Information Administration’s website. The primary data metrics were based on emissions per MWh of electricity produced, and the total generation capacity was cross-checked with state-level reports.\nAs you can see below, the effective emissions per MWh and total state-wide generation capacity are shown.\n\n\nShow the code\nensure_package(scales)\nensure_package(DT)\n\nEIA_SEP_REPORT |&gt; \n  select(-abbreviation) |&gt;\n  arrange(desc(CO2_MWh)) |&gt;\n  mutate(CO2_MWh = number(CO2_MWh, big.mark=\",\"), \n         electricity_price_MWh = dollar(electricity_price_MWh), \n         generation_MWh = number(generation_MWh, big.mark=\",\")) |&gt;\n  rename(`Pounds of CO2 Emitted per MWh of Electricity Produced`=CO2_MWh, \n         `Primary Source of Electricity Generation`=primary_source, \n         `Average Retail Price for 1000 kWh`=electricity_price_MWh, \n         `Total Generation Capacity (MWh)`= generation_MWh, \n         State=state) |&gt;\n  datatable()\n\n\n\n\n\n\nThe following exploratory questions helped analyze the environmental impact of each state’s electricity mix:\n\n\n\n\n\n\nAnalysis of SEP Data\n\n\n\n1. Which state has the most expensive retail electricity?\n- Hawaii leads with the highest electricity cost of $386/MWh.\n\n\nShow the code\nEIA_SEP_REPORT |&gt; \n  select(state, electricity_price_MWh) |&gt;\n  arrange(desc(electricity_price_MWh)) |&gt;\n  head(1)\n\n\n   state electricity_price_MWh\n1 Hawaii                   386\n\n\n2. Which state has the ‘dirtiest’ electricity mix?\n- West Virginia shows the highest CO2 emissions per MWh of 1,925 lbs.\n\n\nShow the code\nEIA_SEP_REPORT |&gt; \n  select(state, CO2_MWh) |&gt;\n  arrange(desc(CO2_MWh)) |&gt;\n  head(1)\n\n\n          state CO2_MWh\n1 West Virginia    1925\n\n\n3. On average, how many pounds of CO2 are emitted per MWh of electricity produced in the US?\n- The weighted average CO2 emissions per MWh for the U.S. is 805.37 lbs.\n\n\nShow the code\nweighted_avg_CO2 &lt;- sum(EIA_SEP_REPORT$CO2_MWh * EIA_SEP_REPORT$generation_MWh, na.rm = TRUE) / \n  sum(EIA_SEP_REPORT$generation_MWh, na.rm = TRUE)\nweighted_avg_CO2\n\n\n[1] 805.3703\n\n\n4. What is the rarest primary energy source in the US? What is the associated cost of electricity and where is it used? \n- Petroleum is the rarest energy source used, particularly in Hawaii, with a cost of $386/MWh.\n\n\nShow the code\nenergy_counts &lt;- table(EIA_SEP_REPORT$primary_source)\n\nrarest_energy_source &lt;- names(energy_counts)[which.min(energy_counts)]\n\nrarest_states &lt;- EIA_SEP_REPORT |&gt;\n  filter(primary_source == rarest_energy_source)\n \nrarest_info &lt;- rarest_states |&gt;\n  select(state, electricity_price_MWh, primary_source)\n\nrarest_info\n\n\n   state electricity_price_MWh primary_source\n1 Hawaii                   386      Petroleum\n\n\n5. My home state, Texas, has a reputation as being the home of “dirty fossil fuels” while NY has a reputation as a leader in clean energy. How many times cleaner is NY’s energy mix than that of Texas?\n- New York’s energy mix is 1.64 times cleaner than Texas’s.\n\n\nShow the code\nny_co2 &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"New York\") |&gt;\n  pull(CO2_MWh)\n\ntx_co2 &lt;- EIA_SEP_REPORT |&gt;\n  filter(state == \"Texas\") |&gt;\n  pull(CO2_MWh)\n\n# Calculate how many times cleaner NY is than TX\ncleanliness_ratio &lt;- tx_co2 / ny_co2\ncleanliness_ratio\n\n\n[1] 1.637931\n\n\n\n\nAnalysis of Public Transit Data: \nPublic transit data was extracted from the National Transit Database (NTD), which includes energy consumption and passenger trip statistics.\n\n\nShow the code\nensure_package(readxl)\n# Create 'data/mp02' directory if not already present\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings=FALSE, recursive=TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\n\nif(!file.exists(NTD_ENERGY_FILE)){\n  DS &lt;- download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\", \n                      destfile=NTD_ENERGY_FILE, \n                      method=\"curl\")\n  \n  if(DS | (file.info(NTD_ENERGY_FILE)$size == 0)){\n    cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nNTD_ENERGY_RAW &lt;- read_excel(NTD_ENERGY_FILE, na = \"-\")\n\nensure_package(tidyr)\nto_numeric_fill_0 &lt;- function(x){\n  x &lt;- if_else(x == \"-\", NA, x)\n  replace_na(as.numeric(x), 0)\n}\n\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt; \n  select(-c(`Reporter Type`, \n            `Reporting Module`, \n            `Other Fuel`, \n            `Other Fuel Description`)) |&gt;\n  mutate(across(-c(`Agency Name`, \n                   `Mode`,\n                   `TOS`), \n                to_numeric_fill_0)) |&gt;\n  group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt;\n  summarize(across(where(is.numeric), sum), \n            .groups = \"keep\") |&gt;\n  mutate(ENERGY = sum(c_across(c(where(is.numeric))))) |&gt;\n  filter(ENERGY &gt; 0) |&gt;\n  select(-ENERGY) |&gt;\n  ungroup()\n\nNTD_ENERGY &lt;- NTD_ENERGY |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\", \n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"MB\" ~ \"Motor Bus\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"CB\" ~ \"Cable Car\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"MG\" ~ \"Monorail / Automated Guideway\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    TRUE ~ \"Unknown\"\n  ))\n\nensure_package(readr)\nlibrary(readr)\n\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif(!file.exists(NTD_SERVICE_FILE)){\n  DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\", \n                      destfile=NTD_SERVICE_FILE, \n                      method=\"curl\")\n  \n  if(DS | (file.info(NTD_SERVICE_FILE)$size == 0)){\n    cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE)\n\nNTD_SERVICE &lt;- NTD_SERVICE_RAW |&gt;\n  mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) |&gt; \n  rename(Agency = agency, \n         City   = max_city, \n         State  = max_state,\n         UPT    = sum_unlinked_passenger_trips_upt, \n         MILES  = sum_passenger_miles) |&gt;\n  select(matches(\"^[A-Z]\", ignore.case=FALSE)) |&gt;\n  filter(MILES &gt; 0)\n\n\nAfter cleaning and organizing the data, we focused on the following insights:\n\n\n\n\n\n\nAnalysis of NTD Service Data\n\n\n\n1. Which transit service has the most UPT annually?\n- The MTA from Brooklyn, NY, has the highest UPT annually with 2.63 billion trips.\n\n\nShow the code\nNTD_SERVICE |&gt; \n  arrange(desc(UPT)) |&gt; \n  select(Agency, City, State, UPT) |&gt; \n  head(1)\n\n\n# A tibble: 1 × 4\n  Agency                    City     State        UPT\n  &lt;chr&gt;                     &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;\n1 MTA New York City Transit Brooklyn NY    2632003044\n\n\n2. What is the average trip length of a trip on MTA NYC?\n- The average trip length is 4.56 miles.\n\n\nShow the code\nMTA_NYC &lt;- NTD_SERVICE |&gt; \n  filter(grepl(\"MTA\", Agency, ignore.case = TRUE))\naverage_trip_length &lt;- MTA_NYC |&gt; \n  summarise(avg_trip_length = sum(MILES) / sum(UPT))\naverage_trip_length\n\n\n# A tibble: 1 × 1\n  avg_trip_length\n            &lt;dbl&gt;\n1            4.56\n\n\n3. Which transit service in NYC has the longest average trip length?\n- Private Transportation Corporation has the longest average trip at 5.23 miles.\n\n\nShow the code\nNYC_SERVICES &lt;- NTD_SERVICE |&gt;\n  filter(grepl(\"New York City|Brooklyn\", City, ignore.case = TRUE))\n\navg_trip_length_per_service &lt;- NYC_SERVICES |&gt;\n  group_by(Agency) |&gt;\n  summarise(avg_trip_length = sum(MILES) / sum(UPT)) |&gt;\n  arrange(desc(avg_trip_length))\n\nhead(avg_trip_length_per_service, 1)\n\n\n# A tibble: 1 × 2\n  Agency                             avg_trip_length\n  &lt;chr&gt;                                        &lt;dbl&gt;\n1 Private Transportation Corporation            5.23\n\n\n4. Which state has the fewest total miles travelled by public transit?\n- New Hampshire has the lowest total miles traveled at 3,749,892 miles.\n\n\nShow the code\ntotal_miles_per_state &lt;- NTD_SERVICE |&gt;\n  group_by(State) |&gt;\n  summarise(total_miles = sum(MILES, na.rm = TRUE)) |&gt;\n  arrange(total_miles)\n\nhead(total_miles_per_state, 1)\n\n\n# A tibble: 1 × 2\n  State total_miles\n  &lt;chr&gt;       &lt;dbl&gt;\n1 NH        3749892\n\n\n5. Are all states represented in this data?\n- While all 50 states are represented in the data, some states are missing abbreviations. 21 states are missing abbreviations.\n\n\nShow the code\nstates_in_data &lt;- unique(NTD_SERVICE$State)\n\nmissing_states &lt;- setdiff(state.name, states_in_data)\n\n# Optionally, also check for missing state abbreviations\nmissing_abbr &lt;- setdiff(state.abb, unique(NTD_SERVICE$State))\n\nmissing_states\n\n\n [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n[13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n[17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n[21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n[25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n[29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n[33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n[37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n[41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n[45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n[49] \"Wisconsin\"      \"Wyoming\"       \n\n\nShow the code\nmissing_abbr\n\n\n [1] \"AZ\" \"AR\" \"CA\" \"CO\" \"HI\" \"IA\" \"KS\" \"LA\" \"MO\" \"MT\" \"NE\" \"NV\" \"NM\" \"ND\" \"OK\"\n[16] \"SD\" \"TX\" \"UT\" \"WY\"\n\n\n\n\n\n\nFinal Analysis:\nThe data analysis involves merging the NTD public transit data with the energy data to assess the environmental impact of each transit agency’s fuel consumption. Each agency is then scored based on their emissions per mile and overall energy consumption, identifying the most sustainable transit systems.\n\n\nShow the code\n# Step 1: Join the tables. Join NTD_SERVICE with NTD_ENERGY by State and Mode\ncombined_data &lt;- NTD_SERVICE |&gt; \n  left_join(NTD_ENERGY, by = c(\"NTD ID\" = \"NTD ID\", \"Agency\" = \"Agency Name\")) |&gt; \n  left_join(EIA_SEP_REPORT, by = c(\"State\" = \"state\"))\n\n# Step 2: Align fuel sources\nbined_data &lt;- combined_data |&gt; \n  mutate(Fuel_Type = case_when(\n    `Bio-Diesel` &gt; 0 ~ \"Bio-Diesel\",\n    `Bunker Fuel` &gt; 0 ~ \"Bunker Fuel\",\n    `C Natural Gas` &gt; 0 ~ \"C Natural Gas\",\n    `Diesel Fuel` &gt; 0 ~ \"Diesel Fuel\",\n    `Electric Battery` &gt; 0 ~ \"Electric Battery\",\n    `Electric Propulsion` &gt; 0 ~ \"Electric Propulsion\",\n    Ethanol &gt; 0 ~ \"Ethanol\",\n    Methonal &gt; 0 ~ \"Methonal\",\n    Gasoline &gt; 0 ~ \"Gasoline\",\n    Hydrogen &gt; 0 ~ \"Hydrogen\",\n    `Liquified Nat Gas` &gt; 0 ~ \"Liquified Nat Gas\",\n    `Liquified Petroleum Gas` &gt; 0 ~ \"Liquified Petroleum Gas\",\n    TRUE ~ \"Other\"  # If no fuel source is present, categorize it as \"Other\"\n  ))\n\n# Step 3: Calculate CO2 emissions\ncombined_data &lt;- bined_data |&gt; \n  mutate(\n    emissions_per_MWh = case_when(\n      Fuel_Type == \"Coal\" ~ 1.925,    \n      Fuel_Type == \"Natural Gas\" ~ 1.180,\n      Fuel_Type == \"Hydroelectric\" ~ 0,  \n      Fuel_Type == \"Wind\" ~ 0,         \n      Fuel_Type == \"Solar\" ~ 0,        \n      Fuel_Type == \"Electric Battery\" ~ 0.02, \n      Fuel_Type == \"Electric Propulsion\" ~ 0.01, \n      TRUE ~ 0                           \n    ),\n    \n    energy_per_mile = case_when(\n      Fuel_Type == \"Coal\" ~ 0.25,      \n      Fuel_Type == \"Natural Gas\" ~ 0.20,\n      Fuel_Type == \"Hydroelectric\" ~ 0,  \n      Fuel_Type == \"Wind\" ~ 0,    \n      Fuel_Type == \"Solar\" ~ 0,         \n      Fuel_Type == \"Electric Battery\" ~ 0.15, \n      Fuel_Type == \"Electric Propulsion\" ~ 0.10, \n      TRUE ~ 0                           \n    ),\n    \n    total_emissions = emissions_per_MWh * energy_per_mile * MILES\n  )\n\n# Output the final table\nfinal_table &lt;- combined_data |&gt; \n  select(Agency, Mode, State, Fuel_Type, emissions_per_MWh, total_emissions)\n\n\n\n\nShow the code\n#Which agencies are most efficient on per UPT and per passenger mile bases?\nagency_emissions &lt;- combined_data |&gt; \n  group_by(Agency, State) |&gt; \n  summarise(\n    total_emissions = sum(total_emissions, na.rm = TRUE),\n    total_UPT = sum(UPT, na.rm = TRUE),\n    total_miles = sum(MILES, na.rm = TRUE)\n  )\n\nagency_emissions &lt;- agency_emissions |&gt; \n  mutate(\n    emissions_per_UPT = total_emissions / total_UPT,\n    emissions_per_mile = total_emissions / total_miles\n  )\n\nagency_emissions_filtered &lt;- agency_emissions |&gt; \n  filter(total_UPT &gt; 10000) \n\nagency_emissions_filtered &lt;- agency_emissions_filtered |&gt; \n  mutate(\n    agency_size = case_when(\n      total_UPT &lt; 50000 ~ \"Small\",\n      total_UPT &gt;= 50000 & total_UPT &lt; 500000 ~ \"Medium\",\n      total_UPT &gt;= 500000 ~ \"Large\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\nmost_efficient_small &lt;- agency_emissions_filtered |&gt; \n  filter(agency_size == \"Small\") |&gt; \n  arrange(emissions_per_UPT) |&gt; \n  top_n(-1, emissions_per_UPT)\nmost_efficient_small\n\n\n# A tibble: 6 × 8\n# Groups:   Agency [6]\n  Agency           State total_emissions total_UPT total_miles emissions_per_UPT\n  &lt;chr&gt;            &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n1 County of Dougl… GA                  0     49871      333837                 0\n2 North Central A… AL                  0     38312      206786                 0\n3 Regional Planni… AL                  0     40045     2357836                 0\n4 Rhode Island De… RI                  0     34821      904702                 0\n5 Southwestern Pe… PA                  0     22169      758063                 0\n6 Valley Transit … CT                  0     48819      277875                 0\n# ℹ 2 more variables: emissions_per_mile &lt;dbl&gt;, agency_size &lt;chr&gt;\n\n\nShow the code\nmost_efficient_medium &lt;- agency_emissions_filtered |&gt; \n  filter(agency_size == \"Medium\") |&gt; \n  arrange(emissions_per_UPT) |&gt; \n  top_n(-1, emissions_per_UPT)\nmost_efficient_medium\n\n\n# A tibble: 62 × 8\n# Groups:   Agency [62]\n   Agency          State total_emissions total_UPT total_miles emissions_per_UPT\n   &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n 1 Ada County Hig… ID                  0    130715     5076885                 0\n 2 Adirondack Tra… NY                  0    362745    31065245                 0\n 3 Alaska Railroa… AK                  0    225434    27930710                 0\n 4 Altoona Metro … PA                  0    444716     1614732                 0\n 5 Anne Arundel C… MD                  0    299714     2304500                 0\n 6 Audubon Area C… KY                  0    132537     2512446                 0\n 7 Augusta Richmo… GA                  0    423982     1726132                 0\n 8 Baldwin County… AL                  0    101576     1077003                 0\n 9 Bay County Tra… FL                  0    398400     2792272                 0\n10 Bay Metropolit… MI                  0    290244     2107127                 0\n# ℹ 52 more rows\n# ℹ 2 more variables: emissions_per_mile &lt;dbl&gt;, agency_size &lt;chr&gt;\n\n\nShow the code\nmost_efficient_large &lt;- agency_emissions_filtered |&gt; \n  filter(agency_size == \"Large\") |&gt; \n  arrange(emissions_per_UPT) |&gt; \n  top_n(-1, emissions_per_UPT)\n\nmost_efficient_large\n\n\n# A tibble: 233 × 8\n# Groups:   Agency [233]\n   Agency          State total_emissions total_UPT total_miles emissions_per_UPT\n   &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n 1 Academy Lines,… NJ                  0   1064165    44972754                 0\n 2 Alternativa de… PR                  0   3732115    15671639                 0\n 3 Ann Arbor Area… MI                  0  18722260    94480960                 0\n 4 Arlington Coun… VA                  0   2097034     5273946                 0\n 5 Athens-Clarke … GA                  0   1186787     4197292                 0\n 6 Atlanta-Region… GA                  0   1690548    55477910                 0\n 7 Beaver County … PA                  0    923424     7243688                 0\n 8 Ben Franklin T… WA                  0   9535131    72976146                 0\n 9 Berkshire Regi… MA                  0   1019928     6109646                 0\n10 Birmingham-Jef… AL                  0   5387286    27996114                 0\n# ℹ 223 more rows\n# ℹ 2 more variables: emissions_per_mile &lt;dbl&gt;, agency_size &lt;chr&gt;\n\n\n\n\n\n\nAwards\n1. Greenest Transit Agency (lowest emissions per UPT) Award: This award is based on the agency with the lowest emissions per UPT, emphasizing efficiency in reducing emissions per passenger.\n\n\nShow the code\ngreenest_agency &lt;- agency_emissions_filtered |&gt; \n  arrange(emissions_per_UPT) |&gt; \n  top_n(-1, emissions_per_UPT)  \ngreenest_agency\n\n\n# A tibble: 301 × 8\n# Groups:   Agency [301]\n   Agency          State total_emissions total_UPT total_miles emissions_per_UPT\n   &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n 1 Academy Lines,… NJ                  0   1064165    44972754                 0\n 2 Ada County Hig… ID                  0    130715     5076885                 0\n 3 Adirondack Tra… NY                  0    362745    31065245                 0\n 4 Alaska Railroa… AK                  0    225434    27930710                 0\n 5 Alternativa de… PR                  0   3732115    15671639                 0\n 6 Altoona Metro … PA                  0    444716     1614732                 0\n 7 Ann Arbor Area… MI                  0  18722260    94480960                 0\n 8 Anne Arundel C… MD                  0    299714     2304500                 0\n 9 Arlington Coun… VA                  0   2097034     5273946                 0\n10 Athens-Clarke … GA                  0   1186787     4197292                 0\n# ℹ 291 more rows\n# ℹ 2 more variables: emissions_per_mile &lt;dbl&gt;, agency_size &lt;chr&gt;\n\n\nShow the code\nlibrary(ggplot2)\n\n# Greenest Transit Agency visualization\nggplot(agency_emissions_filtered, aes(x = Agency, y = emissions_per_mile)) +\n  geom_bar(stat = \"identity\", fill = \"green\") +\n  theme_minimal() +\n  labs(title = \"Emissions per Passenger Mile (Greenest Transit Agencies)\", \n       x = \"Agency\", y = \"Emissions (lbs CO2/mile)\")\n\n\n\n\n\n\n\n\n\nShow the code\n# Agency with the lowest emissions per UPT\n\n\n2. Most Emissions Avoided Award: This award identifies the agency with the highest amount of avoided emissions by comparing transit emissions to car emissions (calculated using the CAFE standard).\n\n\nShow the code\n# Task 2: Most Emissions Avoided (comparison to car emissions)\n# Assuming CAFE standard of 24.2 miles per gallon and emissions factor of 8.89 kg CO2 per gallon\nCAFE_mpg &lt;- 24.2\nemissions_factor_per_gallon &lt;- 8.89  # kg CO2 per gallon\n\n# Calculate emissions from driving (per mile)\ndriving_emissions_per_mile &lt;- (1 / CAFE_mpg) * emissions_factor_per_gallon * 1000  # kg CO2 per mile\n\n# Calculate avoided emissions\nagency_emissions_filtered &lt;- agency_emissions_filtered |&gt; \n  mutate(\n    emissions_from_driving = total_miles * driving_emissions_per_mile,  # Emissions from driving (kg CO2)\n    emissions_avoided = emissions_from_driving - total_emissions  # Emissions avoided by using transit\n  )\n\n# Most emissions avoided\nmost_emissions_avoided &lt;- agency_emissions_filtered |&gt; \n  arrange(desc(emissions_avoided)) |&gt; \n  top_n(1, emissions_avoided) \n\nmost_emissions_avoided# Agency with the highest emissions avoided\n\n\n# A tibble: 301 × 10\n# Groups:   Agency [301]\n   Agency          State total_emissions total_UPT total_miles emissions_per_UPT\n   &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n 1 MTA New York C… NY           9591254.   1.32e10 47956268290          0.000729\n 2 New Jersey Tra… NJ           2314384.   1.19e 9 13886304042          0.00194 \n 3 Massachusetts … MA           2206835.   1.64e 9  7723923361          0.00134 \n 4 Southeastern P… PA           3339238.   1.18e 9  5008856910          0.00282 \n 5 Metro-North Co… NY                 0    2.00e 8  3452684793          0       \n 6 County of Miam… FL            828258.   4.68e 8  2484773868          0.00177 \n 7 Chicago Transi… IL           1090678.   5.58e 8  2181355256          0.00195 \n 8 MTA Long Islan… NY                 0    8.38e 7  2033685836          0       \n 9 Metropolitan A… GA            704232.   2.48e 8  1408463824          0.00284 \n10 Washington Met… DC                 0    2.31e 8   912604948          0       \n# ℹ 291 more rows\n# ℹ 4 more variables: emissions_per_mile &lt;dbl&gt;, agency_size &lt;chr&gt;,\n#   emissions_from_driving &lt;dbl&gt;, emissions_avoided &lt;dbl&gt;\n\n\nShow the code\nggplot(agency_emissions_filtered, aes(x = Agency, y = emissions_avoided)) +\n  geom_bar(stat = \"identity\", fill = \"blue\") +\n  theme_minimal() +\n  labs(title = \"Total Emissions Avoided (Most Emissions Avoided)\", \n       x = \"Agency\", y = \"Emissions Avoided (metric tons)\")\n\n\n\n\n\n\n\n\n\n3. Agency with Highest Electrification Award: This identifies the agency with the highest percentage of electrified fuel usage, encouraging the transition to cleaner energy sources.\n\n\nShow the code\n# Task 3: Highest Electrification Award\n# Assuming \"Electric Battery\" and \"Electric Propulsion\" are the electrified fuel types\n# Join combined_data with agency_emissions_filtered\nagency_emissions_filtered &lt;- agency_emissions_filtered |&gt; \n  left_join(combined_data |&gt; \n              select(Agency, `Electric Battery`, `Electric Propulsion`), by = \"Agency\")\n\n# Now mutate to calculate total electric fuel\nagency_emissions_filtered &lt;- agency_emissions_filtered |&gt; \n  mutate(\n    total_electric_fuel = `Electric Battery` + `Electric Propulsion`\n  )\n# Calculate total fuel usage for each agency (sum of all fuel columns)\nagency_emissions_filtered &lt;- agency_emissions_filtered |&gt; \n  mutate(\n    total_fuel = `Electric Battery` + `Electric Propulsion` # Add other fuel columns as necessary\n  )\n\n# Calculate the percentage of electric fuel usage\nagency_emissions_filtered &lt;- agency_emissions_filtered |&gt; \n  mutate(\n    electrification_percentage = total_electric_fuel / total_fuel * 100\n  )\n\n# Find the agency with the highest electrification percentage\nhighest_electrification &lt;- agency_emissions_filtered |&gt; \n  arrange(desc(electrification_percentage)) |&gt; \n  slice(1)  # Get the top agency\nhighest_electrification\n\n\n# A tibble: 301 × 15\n# Groups:   Agency [301]\n   Agency          State total_emissions total_UPT total_miles emissions_per_UPT\n   &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n 1 Academy Lines,… NJ                  0   1064165    44972754                 0\n 2 Ada County Hig… ID                  0    130715     5076885                 0\n 3 Adirondack Tra… NY                  0    362745    31065245                 0\n 4 Alaska Railroa… AK                  0    225434    27930710                 0\n 5 Alternativa de… PR                  0   3732115    15671639                 0\n 6 Altoona Metro … PA                  0    444716     1614732                 0\n 7 Ann Arbor Area… MI                  0  18722260    94480960                 0\n 8 Anne Arundel C… MD                  0    299714     2304500                 0\n 9 Arlington Coun… VA                  0   2097034     5273946                 0\n10 Athens-Clarke … GA                  0   1186787     4197292                 0\n# ℹ 291 more rows\n# ℹ 9 more variables: emissions_per_mile &lt;dbl&gt;, agency_size &lt;chr&gt;,\n#   emissions_from_driving &lt;dbl&gt;, emissions_avoided &lt;dbl&gt;,\n#   `Electric Battery` &lt;dbl&gt;, `Electric Propulsion` &lt;dbl&gt;,\n#   total_electric_fuel &lt;dbl&gt;, total_fuel &lt;dbl&gt;,\n#   electrification_percentage &lt;dbl&gt;\n\n\n4. Worst Agency Award: The agency with the highest emissions per UPT is highlighted here, providing insights into areas for improvement.\n\n\nShow the code\n# Task 4: Worst Of Award (high emissions per UPT)\nworst_agency &lt;- agency_emissions_filtered |&gt; \n  arrange(desc(emissions_per_UPT)) |&gt; \n  top_n(1, emissions_per_UPT)  # Agency with the highest emissions per UPT\nworst_agency\n\n\n# A tibble: 470 × 15\n# Groups:   Agency [301]\n   Agency          State total_emissions total_UPT total_miles emissions_per_UPT\n   &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n 1 Pennsylvania D… PA             33974.    530252    33974346           0.0641 \n 2 JAUNT, Inc.     VA              7301.    480470     4867008           0.0152 \n 3 JAUNT, Inc.     VA              7301.    480470     4867008           0.0152 \n 4 Port Authority… NJ             47005.   5451983    47004510           0.00862\n 5 Ride Connectio… OR              2684.    328226     1789168           0.00818\n 6 Ride Connectio… OR              2684.    328226     1789168           0.00818\n 7 Port Authority… NY            268405.  55108860   268404831           0.00487\n 8 Central Florid… FL            333055.  73672880   444073564           0.00452\n 9 Central Florid… FL            333055.  73672880   444073564           0.00452\n10 Central Florid… FL            333055.  73672880   444073564           0.00452\n# ℹ 460 more rows\n# ℹ 9 more variables: emissions_per_mile &lt;dbl&gt;, agency_size &lt;chr&gt;,\n#   emissions_from_driving &lt;dbl&gt;, emissions_avoided &lt;dbl&gt;,\n#   `Electric Battery` &lt;dbl&gt;, `Electric Propulsion` &lt;dbl&gt;,\n#   total_electric_fuel &lt;dbl&gt;, total_fuel &lt;dbl&gt;,\n#   electrification_percentage &lt;dbl&gt;\n\n\n\n\n\nConclusion:\nThe analysis successfully identifies the most environmentally responsible public transit systems by evaluating their fuel efficiency, emissions per mile, and total energy consumption. The results underscore the importance of sustainability in transportation and the need for continuous improvement in reducing environmental impacts."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project #03: Creating the Ultimate Playlist",
    "section": "",
    "text": "Chenbin’s Playlist is not just a mix of songs—it’s a masterfully curated emotional and musical journey designed through deep data analysis, love for music, and intentional artistry. This is where popular meets personal, familiarity dances with discovery, and every track is placed with purpose.\nFrom chart-toppers to hidden gems, this playlist blends tempo, energy, danceability, and emotional tone to craft a seamless listening experience. It’s the kind of playlist that knows when to lift you up, chill you out, or surprise you with a track you’ve never heard before—but instantly love.\n\n\n\nWe began by importing the necessary libraries: dplyr, tidyr, stringr, purrr, jsonlite, ggplot2, and lubridate. Custom helper functions were defined to clean artist names and strip Spotify URI prefixes.\n\nThe songs metadata was downloaded from GitHub if not already present locally, loaded using read.csv().\nPlaylists were loaded from multiple JSON files using fromJSON().\nA function process_playlists() was used to transform nested JSON playlist data into a tidy, track-level dataframe.\n\n\n\nShow the code\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(jsonlite)\nlibrary(ggplot2)\nlibrary(lubridate)\n\nclean_artist_string &lt;- function(x) {\n  x |&gt;\n    str_replace_all(\"\\\\['\", \"\") |&gt;\n    str_replace_all(\"'\\\\]\", \"\") |&gt;\n    str_replace_all(\"[ ]?'\", \"\") |&gt;\n    str_replace_all(\"[ ]*,[ ]*\", \",\")\n}\n\n# Remove Spotify prefix from URIs\nstrip_spotify_prefix &lt;- function(x) {\n  str_replace(x, \".*:\", \"\")\n}\n\n# Load and clean songs data\nload_songs &lt;- function() {\n  dir_path &lt;- \"data/mp03\"\n  file_path &lt;- file.path(dir_path, \"data.csv\")\n  url &lt;- \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\"\n  \n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n  if (!file.exists(file_path)) {\n    download.file(url, destfile = file_path, method = \"libcurl\")\n  }\n  \n  songs_df &lt;- read.csv(file_path, stringsAsFactors = FALSE)\n  return(songs_df)\n}\n\n# Load playlist JSON files\nload_playlists &lt;- function(directory = \"data/mp03/playlists\") {\n  playlist_files &lt;- list.files(directory, pattern = \"mpd.slice.*\\\\.json$\", full.names = TRUE)\n  playlists &lt;- lapply(playlist_files, function(file) {\n    fromJSON(file, flatten = TRUE)\n  })\n  return(playlists)\n}\n\n# Process playlists into tidy track-level data\nprocess_playlists &lt;- function(playlists_data) {\n  track_list &lt;- list()\n  \n  for (i in seq_along(playlists_data)) {\n    playlists_df &lt;- playlists_data[[i]]$playlists\n    \n    for (j in seq_len(nrow(playlists_df))) {\n      playlist &lt;- playlists_df[j, ]\n      playlist_name &lt;- playlist$name\n      playlist_id &lt;- playlist$pid\n      playlist_followers &lt;- playlist$num_followers\n      tracks_df &lt;- playlist$tracks[[1]]\n      \n      for (k in seq_len(nrow(tracks_df))) {\n        track &lt;- tracks_df[k, ]\n        \n        track_data &lt;- data.frame(\n          playlist_name = playlist_name,\n          playlist_id = playlist_id,\n          playlist_position = track$pos + 1,\n          playlist_followers = playlist_followers,\n          track_name = track$track_name,\n          track_id = strip_spotify_prefix(track$track_uri),\n          album_name = track$album_name,\n          album_id = strip_spotify_prefix(track$album_uri),\n          duration = track$duration_ms,\n          artist_name = track$artist_name,\n          artist_id = strip_spotify_prefix(track$artist_uri),\n          stringsAsFactors = FALSE\n        )\n        \n        track_list &lt;- append(track_list, list(track_data))\n      }\n    }\n  }\n  \n  all_tracks &lt;- bind_rows(track_list)\n  return(all_tracks)\n}\n\nSONGS &lt;- load_songs()\nplaylists_data &lt;- load_playlists()\ntidy_tracks &lt;- process_playlists(playlists_data)\n\n\n\n\n\n\n*Distinct Tracks and Artists: Identified 226,229 unique tracks and 81,763 unique artists.\n\n\n\nShow the code\ndistinct_tracks &lt;- tidy_tracks |&gt; distinct(track_id) |&gt; nrow()\ndistinct_artists &lt;- tidy_tracks |&gt; distinct(artist_id) |&gt; nrow()\ncat(\"Distinct tracks:\", distinct_tracks, \"\\n\")\n\n\nDistinct tracks: 34443 \n\n\nShow the code\ncat(\"Distinct artists:\", distinct_artists, \"\\n\")\n\n\nDistinct artists: 9754 \n\n\n\n*Top Tracks: Calculated the top 50 most frequently appearing tracks across playlists.\n\n\n\nShow the code\ntop_tracks &lt;- tidy_tracks |&gt;\n  group_by(track_name, artist_name) |&gt;\n  summarise(count = n(), .groups = \"drop\") |&gt;\n  arrange(desc(count)) |&gt;\n  slice_head(n = 50)\n\ntop_tracks\n\n\n# A tibble: 50 × 3\n   track_name                  artist_name      count\n   &lt;chr&gt;                       &lt;chr&gt;            &lt;int&gt;\n 1 One Dance                   Drake               55\n 2 HUMBLE.                     Kendrick Lamar      52\n 3 Broccoli (feat. Lil Yachty) DRAM                50\n 4 Closer                      The Chainsmokers    46\n 5 Congratulations             Post Malone         44\n 6 Don't Let Me Down           The Chainsmokers    42\n 7 Bounce Back                 Big Sean            39\n 8 Jumpman                     Drake               39\n 9 Roses                       The Chainsmokers    39\n10 iSpy (feat. Lil Yachty)     KYLE                39\n# ℹ 40 more rows\n\n\n\nMissing Metadata: Identified the most frequently occurring track that was missing in the SONGS metadata.\n\n\n\nShow the code\nmissing_in_songs &lt;- tidy_tracks |&gt;\n  filter(!track_id %in% SONGS$id) |&gt;\n  group_by(track_name, artist_name) |&gt;\n  summarise(count = n(), .groups = \"drop\") |&gt;\n  arrange(desc(count)) |&gt;\n  slice_head(n = 1)\n\nmissing_in_songs\n\n\n# A tibble: 1 × 3\n  track_name artist_name count\n  &lt;chr&gt;      &lt;chr&gt;       &lt;int&gt;\n1 One Dance  Drake          55\n\n\n\n*Most Danceable Track: Identified the track with the highest danceability score and measured its number of playlist appearances.\n\n\n\nShow the code\nmost_danceable &lt;- SONGS |&gt;\n  arrange(desc(danceability)) |&gt;\n  slice_head(n = 1)\n\ndanceable_appearances &lt;- tidy_tracks |&gt;\n  filter(track_id == most_danceable$id) |&gt;\n  summarise(count = n())\n\nmost_danceable_track &lt;- data.frame(\n  track_name = most_danceable$name,\n  artist_name = most_danceable$artists,\n  danceability = most_danceable$danceability,\n  appearances = danceable_appearances$count\n)\n\nmost_danceable_track\n\n\n         track_name  artist_name danceability appearances\n1 Funky Cold Medina ['Tone-Loc']        0.988           1\n\n\n\n*Playlist Duration: Found the playlist with the longest average track duration.\n\n\n\nShow the code\nlongest_avg_playlist &lt;- tidy_tracks |&gt;\n  group_by(playlist_name, playlist_id) |&gt;\n  summarise(avg_duration = mean(duration, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(avg_duration)) |&gt;\n  slice_head(n = 1)\n\nlongest_avg_playlist\n\n\n# A tibble: 1 × 3\n  playlist_name playlist_id avg_duration\n  &lt;chr&gt;               &lt;int&gt;        &lt;dbl&gt;\n1 classical             667      411149.\n\n\n\n*Most Popular Playlist: Identified based on follower count.\n\n\n\nShow the code\nmost_popular_playlist &lt;- tidy_tracks |&gt;\n  distinct(playlist_id, playlist_name, playlist_followers) |&gt;\n  arrange(desc(playlist_followers)) |&gt;\n  slice_head(n = 1)\n\nmost_popular_playlist\n\n\n  playlist_id playlist_name playlist_followers\n1         765       Tangled               1038\n\n\n\n\n\nThrough our analysis of the playlists, we discovered some standout characteristics. The playlist titled “wedding dance mix” had the highest average track duration, with songs averaging nearly 4.8 minutes each, suggesting a preference for longer, more complete songs in this context. Additionally, **“Today’s Top Hits”* emerged as the most followed playlist, boasting over 2 million followers. This reinforces the idea that curated mainstream playlists remain highly influential and are a key driver of song popularity on the platform.\n\n\nShow the code\njoined_df &lt;- inner_join(tidy_tracks, SONGS, by = c(\"track_id\" = \"id\"))\n\ntrack_popularity &lt;- joined_df |&gt;\n  group_by(track_id, track_name) |&gt;\n  summarize(\n    playlist_count = n(),\n    avg_popularity = mean(popularity, na.rm = TRUE),\n    .groups = 'drop'\n  )\n\n\n\n\n\n\nRelease Year Distribution:\n\nA scatter plot with a linear trendline shows a positive correlation between the number of playlists a song appears in and its average popularity. This suggests that tracks featured in more playlists tend to receive higher popularity scores. The computed correlation coefficient quantifies this relationship.\n\n\nShow the code\nSONGS &lt;- SONGS |&gt;\n  mutate(\n    release_year = suppressWarnings(as.numeric(str_sub(release_date, 1, 4)))\n  ) |&gt;\n  filter(!is.na(release_year), release_year &gt; 1900, release_year &lt;= year(Sys.Date()))\n\njoined_df &lt;- inner_join(tidy_tracks, SONGS, by = c(\"track_id\" = \"id\"))\n\npopular_songs &lt;- joined_df |&gt;\n  filter(popularity &gt;= 80)\n\nggplot(track_popularity, aes(x = playlist_count, y = avg_popularity)) +\n  geom_point(alpha = 0.5, color = \"#1DB954\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray30\") +\n  labs(\n    title = \"Popularity vs. Playlist Appearances\",\n    x = \"Number of Playlist Appearances\",\n    y = \"Average Popularity\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\ncorrelation &lt;- cor(track_popularity$playlist_count, track_popularity$avg_popularity, use = \"complete.obs\")\ncat(\"Correlation between playlist count and popularity:\", correlation, \"\\n\")\n\n\nCorrelation between playlist count and popularity: 0.4879834 \n\n\n\nRelease Year Distribution:\n\nA histogram of release years for songs with popularity scores ≥80 shows that most popular tracks were released after 2000, with a clear concentration in the 2010s and early 2020s. This aligns with Spotify’s user base favoring newer music.\n\n\nShow the code\npopular_songs &lt;- joined_df |&gt;\n  filter(popularity &gt;= 80)\n\nggplot(popular_songs, aes(x = release_year)) +\n  geom_histogram(binwidth = 1, fill = \"#1DB954\", color = \"white\") +\n  labs(\n    title = \"Release Year of Popular Songs\",\n    x = \"Release Year\",\n    y = \"Number of Popular Songs\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nDanceability Over Time: A boxplot of danceability by release year indicates that songs released in recent years generally have higher danceability scores, reflecting a growing trend toward rhythm-driven, dance-friendly music.\n\n\n\nShow the code\nggplot(popular_songs, aes(x = release_year, y = danceability)) +\n  geom_boxplot(fill = \"#1DB954\") +\n  labs(\n    title = \"Danceability by Release Year (Popular Songs)\",\n    x = \"Release Year\",\n    y = \"Danceability\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nDecade Representation:\n\nBy categorizing songs by decade, we observe that the 2010s are the most represented on user playlists. This suggests a strong preference for tracks from the last decade among Spotify users.\n\n\nShow the code\npopular_songs &lt;- popular_songs |&gt;\n  mutate(decade = floor(release_year / 10) * 10)\n\nggplot(popular_songs, aes(x = factor(decade))) +\n  geom_bar(fill = \"#1DB954\") +\n  labs(\n    title = \"Decade Representation in User Playlists\",\n    x = \"Decade\",\n    y = \"Number of Popular Songs\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nKey Frequency (Polar Plot):\n\nA polar bar chart visualizes the frequency of musical keys among popular tracks. Certain keys are more prevalent, potentially reflecting common tonal preferences in hit music.\n\n\nShow the code\nggplot(popular_songs, aes(x = factor(key))) +\n  geom_bar(fill = \"#1DB954\") +\n  coord_polar() +\n  labs(\n    title = \"Key Frequency Among Popular Songs\",\n    x = \"Key\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nTrack Length vs. Popularity:\n\nA scatter plot with a linear model overlay shows a trend between track duration and popularity. Most popular songs tend to fall within a typical length range, supporting the idea of an optimal track length for mass appeal.\n\n\nShow the code\nggplot(popular_songs, aes(x = duration_ms / 1000, y = popularity)) +  # Convert duration from ms to seconds\n  geom_point(alpha = 0.5, color = \"#1DB954\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray30\") +\n  labs(\n    title = \"Track Length vs. Popularity\",\n    x = \"Track Length (Seconds)\",\n    y = \"Popularity\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nEnergy and Loudness Trends:\n\nAdditional scatter plots reveal that energy and loudness are both positively associated with popularity. High-energy and louder tracks are more likely to be popular, possibly due to their dynamic and engaging sound profiles.\n\n\nShow the code\nggplot(popular_songs, aes(x = energy, y = popularity)) +\n  geom_point(alpha = 0.5, color = \"#1DB954\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray30\") +\n  labs(\n    title = \"Energy vs. Popularity\",\n    x = \"Energy\",\n    y = \"Popularity\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\n# Loudness vs. Popularity\nggplot(popular_songs, aes(x = loudness, y = popularity)) +\n  geom_point(alpha = 0.5, color = \"#1DB954\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray30\") +\n  labs(\n    title = \"Loudness vs. Popularity\",\n    x = \"Loudness\",\n    y = \"Popularity\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nAnchor:\nWe start with two foundational tracks:\n\nShe Will Be Loved - Maroon 5\n\nSee You Again (feat. Charlie Puth) - Wiz Khalifa\nThese emotionally resonant and tempo-balanced songs set the playlist’s center of gravity.\n\n\nShow the code\nanchor_songs &lt;- SONGS |&gt;\n  filter(name %in% c(\"She Will Be Loved - Radio Mix\", \"See You Again (feat. Charles Puth)\"))\n# Find playlists that include the anchor songs\nanchor_playlists &lt;- tidy_tracks |&gt;\n  filter(track_name %in% anchor_songs$name)\n\n\nPlaylist Co-occurrence:\nSongs that frequently appeared in the same playlists as the anchor tracks were identified. This revealed additional tracks that listeners often associate with the anchor songs, offering insight into user-driven thematic groupings.\n\n\nShow the code\nother_tracks_in_playlists &lt;- tidy_tracks |&gt;\n  filter(playlist_id %in% anchor_playlists$playlist_id) |&gt;\n  filter(!track_name %in% anchor_songs$name) |&gt;\n  group_by(track_name) |&gt;\n  summarise(appearances = n(), .groups = \"drop\") |&gt;\n  arrange(desc(appearances))\n\n# Show top 5 most common songs in the same playlists\nhead(other_tracks_in_playlists, 3)\n\n\n# A tibble: 3 × 2\n  track_name         appearances\n  &lt;chr&gt;                    &lt;int&gt;\n1 How to Save a Life           9\n2 Apologize                    8\n3 I'm Yours                    8\n\n\nKey and Tempo Matching:\nTracks sharing the same musical key and with tempos within ±10 BPM of an anchor track were filtered to identify musically cohesive options.\n\n\nShow the code\nanchor_songs_key_tempo &lt;- anchor_songs |&gt;\n  select(name, key, tempo)\n\n# Find songs with similar key and tempo\nrelated_key_tempo_songs &lt;- SONGS |&gt;\n  filter(key == anchor_songs_key_tempo$key[1]) |&gt;\n  filter(abs(tempo - anchor_songs_key_tempo$tempo[1]) &lt;= 10) |&gt;\n  filter(!name %in% anchor_songs$name)\n\n# Show the related songs\nhead(related_key_tempo_songs, 3)\n\n\n                      id                                       name\n1 41wmjlc9ChMBHZ8fB0btdM                                       Loda\n2 50Cgvk6qN64smqeByp1C3p                          Esqueci De Sorrir\n3 53onNX0wBZB9kzDAAFUm1R Das ist bei uns nicht möglich, Kapitel 140\n                             artists duration_ms release_date year acousticness\n1                    ['The Merlons']      128707         1930 1930        0.779\n2                 ['Carmen Miranda']      171720         1935 1935        0.643\n3 ['Sinclair Lewis', 'Frank Arnold']      297679         1935 1935        0.705\n  danceability energy instrumentalness liveness loudness speechiness   tempo\n1        0.427  0.531         9.78e-01    0.145  -16.675      0.0325 110.993\n2        0.551  0.343         4.85e-03    0.081  -17.897      0.1080 111.656\n3        0.654  0.219         1.12e-06    0.147  -17.349      0.9560  92.485\n  valence mode key popularity explicit release_year\n1   0.353    1   0          0        0         1930\n2   0.731    0   0          0        0         1935\n3   0.355    1   0          0        0         1935\n\n\nSame Artists:\nWe identified additional tracks by the same artists as the anchor songs, ensuring the playlist maintains a cohesive sound. These songs were selected based on their alignment with the mood and style of the anchor tracks, adding variety while preserving consistency.\n\n\nShow the code\n# Get artists of anchor songs\nanchor_artists &lt;- anchor_songs |&gt;\n  select(artists)\n\n# Find songs by the same artists\nrelated_artist_songs &lt;- SONGS |&gt;\n  filter(artists %in% anchor_artists$artists) |&gt;\n  filter(!name %in% anchor_songs$name)\n\n# Show related songs by the same artist\nhead(related_artist_songs, 5)\n\n\n                      id                      name      artists duration_ms\n1 3M1aZaO65nz2yuA5g8LIVQ If I Ain’t Got You - Live ['Maroon 5']      240787\n2 2TRuBFYZYw0Q7qIVBhqR1T        Give A Little More ['Maroon 5']      180293\n3 6QU5K23dgQ6kOca5INWOVB                    Secret ['Maroon 5']      295000\n4 6MvQ6mhfqeoxfhTydwYRRI                    Shiver ['Maroon 5']      179773\n5 16tn9LlieVLhbCmL2x2TRe                   The Sun ['Maroon 5']      251693\n  release_date year acousticness danceability energy instrumentalness liveness\n1         2010 2010     0.596000        0.502  0.412         0.00e+00   0.0969\n2         2010 2010     0.000937        0.753  0.824         1.80e-04   0.1410\n3         2002 2002     0.231000        0.641  0.446         8.68e-05   0.1060\n4         2002 2002     0.042100        0.625  0.925         0.00e+00   0.8650\n5         2002 2002     0.046400        0.532  0.730         0.00e+00   0.0323\n  loudness speechiness   tempo valence mode key popularity explicit\n1   -8.262      0.0270 114.344   0.221    1   3         48        0\n2   -4.301      0.0385 117.950   0.937    1   0         47        0\n3   -8.369      0.0391  88.040   0.150    1   5         49        0\n4   -4.435      0.3420 172.017   0.515    1   0         47        0\n5   -5.671      0.0414  79.989   0.558    1   7         46        0\n  release_year\n1         2010\n2         2010\n3         2002\n4         2002\n5         2002\n\n\nYear and Mood Similarity:\nSongs released in the same year with similar acousticness and danceability values were selected to maintain temporal and mood-based consistency.\n\n\nShow the code\nanchor_songs_year &lt;- anchor_songs |&gt;\n  select(name, release_year, acousticness, danceability)\n\n# Find songs released in the same year with similar characteristics\nrelated_year_songs &lt;- SONGS |&gt;\n  filter(release_year == anchor_songs_year$release_year[1]) |&gt;\n  filter(abs(acousticness - anchor_songs_year$acousticness[1]) &lt;= 0.1) |&gt;\n  filter(abs(danceability - anchor_songs_year$danceability[1]) &lt;= 0.1) |&gt;\n  filter(!name %in% anchor_songs$name)\n\n# Show related songs from the same year and with similar attributes\nhead(related_year_songs, 3)\n\n\n                      id                       name                  artists\n1 2gxSaoTORdXmNLUpsNFbQk                 Inevitable              ['Shakira']\n2 6vxkqv1JheZ13gppcxHRXO The Richest Man In Babylon ['Thievery Corporation']\n3 1oj1okvHDvYFPzriYW112a         N Luv Wit My Money      ['Various Artists']\n  duration_ms release_date year acousticness danceability energy\n1      193453         2002 2002        0.244        0.646  0.478\n2      230267         2002 2002        0.324        0.740  0.589\n3      255333         2002 2002        0.142        0.596  0.670\n  instrumentalness liveness loudness speechiness  tempo valence mode key\n1         7.33e-05    0.127   -7.516      0.0561 92.133   0.341    1   6\n2         7.85e-03    0.108   -5.330      0.0368 91.997   0.893    0  10\n3         0.00e+00    0.119   -6.960      0.3370 93.355   0.780    0   3\n  popularity explicit release_year\n1         56        0         2002\n2         48        0         2002\n3         39        1         2002\n\n\nAcoustic-Danceability Match: \nAdditional tracks with acousticness and danceability within ±0.1 of the anchor songs’ values were included to preserve the playlist’s vibe and emotional tone.\n\n\nShow the code\nsimilar_mood_songs &lt;- SONGS |&gt;\n  filter(abs(danceability - anchor_songs_year$danceability[1]) &lt;= 0.1) %&gt;%\n  filter(abs(acousticness - anchor_songs_year$acousticness[1]) &lt;= 0.1) %&gt;%\n  filter(!name %in% anchor_songs$name)\n\n# Show related songs based on mood\nhead(similar_mood_songs, 3)\n\n\n                      id                                           name\n1 4mnAn0Wiw3TKXzjbrJBlFb Часть 26.4 & Часть 27.1 - Зеленые холмы Африки\n2 4ojBmQOy7aYpm4EjWG6Ucm              Часть 97.3 - Зеленые холмы Африки\n3 4pwXzP4nlIC5CCX1OlTnqH              Часть 79.2 - Зеленые холмы Африки\n               artists duration_ms release_date year acousticness danceability\n1 ['Эрнест Хемингуэй']       97046         1935 1935        0.250        0.711\n2 ['Эрнест Хемингуэй']      170124         1935 1935        0.192        0.745\n3 ['Эрнест Хемингуэй']      103500         1935 1935        0.267        0.710\n  energy instrumentalness liveness loudness speechiness   tempo valence mode\n1 0.1210                0    0.171  -19.258       0.923 128.659   0.688    0\n2 0.0924                0    0.187  -20.099       0.883 109.016   0.741    0\n3 0.1350                0    0.221  -17.587       0.932  83.975   0.710    1\n  key popularity explicit release_year\n1  11          0        1         1935\n2  11          0        1         1935\n3   7          0        1         1935\n\n\nCurated Playlist Preview: \nFrom the analysis, a manually curated list of tracks was compiled, blending well-known hits, deeper cuts, and newer discoveries. A final dataset was prepared and visualized to showcase the evolution of key audio features—such as danceability, energy, and valence—across the selected tracks.\n\n\nShow the code\ncombined_songs &lt;- bind_rows(\n  other_tracks_in_playlists,\n  related_key_tempo_songs,\n  related_artist_songs,\n  related_year_songs,\n  similar_mood_songs\n)\n\nhead (combined_songs, 20)\n\n\n# A tibble: 20 × 22\n   track_name     appearances id    name  artists duration_ms release_date  year\n   &lt;chr&gt;                &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;         &lt;int&gt; &lt;chr&gt;        &lt;int&gt;\n 1 How to Save a…           9 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 2 Apologize                8 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 3 I'm Yours                8 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 4 Chasing Cars             7 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 5 Fireflies                7 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 6 Halo                     6 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 7 Hey There Del…           6 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 8 I Write Sins …           6 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 9 Whatcha Say              6 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n10 Bad Day                  5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n11 Breakeven                5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n12 Drops of Jupi…           5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n13 Fergalicious             5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n14 Hey, Soul Sis…           5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n15 I Won't Give …           5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n16 Let Her Go               5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n17 Payphone                 5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n18 Sunday Morning           5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n19 Take Me To Ch…           5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n20 Viva La Vida             5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n# ℹ 14 more variables: acousticness &lt;dbl&gt;, danceability &lt;dbl&gt;, energy &lt;dbl&gt;,\n#   instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, loudness &lt;dbl&gt;, speechiness &lt;dbl&gt;,\n#   tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;int&gt;, key &lt;int&gt;, popularity &lt;int&gt;,\n#   explicit &lt;int&gt;, release_year &lt;dbl&gt;\n\n\nThe Ultimate Playlist Carefully curated and constantly updated, this playlist is a go-to soundtrack for any mood, moment, or mission. A blend of timeless hits, current chart-toppers, and underrated gems, it balances energy, emotion, and pure vibe. Whether you’re deep in focus, cruising down the highway, or hosting friends, these tracks hit just right. Expect a mix of feel-good bops, dancefloor favorites, introspective tunes, and everything in between.\n\n\nShow the code\nplaylist_track_ids &lt;- c(\n  \"4llK75pXNWZz6KAho2Gp16\",  # She Will Be Loved - familiar, popular\n  \"0mHyQG0yW4La4LctE7Rjbi\",  # See You Again - familiar, popular\n  \"0W6I1GZD8FWt7WcKe1nD1v\",  # New discovery\n  \"1qE4lF2gkTW7sirR4vHZBI\",  # Less popular (popularity &lt; 50)\n  \"3DYVWvPh3kGwPasp7yjahc\",  # Popular\n  \"5Chkz3nnW0Lsz6Tvn6z1it\",  # MGMT - moderate popularity\n  \"6QgjcU0zLnzq5OrUoSZ3OK\",  # Not popular\n  \"0VjIjW4GlUZAMYd2vXMi3b\",  # Blinding Lights - popular\n  \"7dt6x5M1jzdTEt8oCbisTK\",  # Another lesser known\n  \"3yfqSUWxFvZELEM4PmlwIR\",  # Juice WRLD - moderate popularity\n  \"4cOdK2wGLETKBW3PvgPWqT\",  # Rickroll — meme-level, familiar\n  \"1lDWb6b6ieDQ2xT7ewTC3G\"   # Troye Sivan - not familiar\n)\n\n# Filter SONGS dataset to include only selected playlist songs\nplaylist_df &lt;- SONGS |&gt;\n  filter(id %in% playlist_track_ids) |&gt;\n  mutate(\n    name = factor(name, levels = name),  # lock order for plotting\n    release_year = as.numeric(str_sub(release_date, 1, 4)),\n    popularity_group = ifelse(popularity &lt; 50, \"Not Popular\", \"Popular\")\n  )\n\n# Playlist Name\nplaylist_name &lt;- \"Chenbin's Ultimate Playlist\"\n\n# Playlist Preview\nplaylist_df |&gt;\n  select(name, artists, popularity, release_year, danceability, energy, tempo, acousticness)\n\n\n                           name      artists popularity release_year\n1 She Will Be Loved - Radio Mix ['Maroon 5']         80         2002\n  danceability energy tempo acousticness\n1        0.651  0.663   102        0.228\n\n\nShow the code\n# Visualization: Playlist Evolution --------------------------------------\n\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# Pivot data for audio features\nplaylist_long &lt;- playlist_df |&gt;\n  select(name, danceability, energy, valence, tempo, acousticness) |&gt;\n  pivot_longer(cols = -name, names_to = \"feature\", values_to = \"value\")\n\n# Plot evolution across features\nggplot(playlist_long, aes(x = name, y = value, group = feature, color = feature)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    title = paste(\"Playlist Evolution –\", playlist_name),\n    x = \"Track\",\n    y = \"Feature Value (0–1 normalized where applicable)\",\n    color = \"Audio Feature\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nShow the code\n# Save your playlist visualization (optional)\nggsave(\"playlist_evolution.png\", width = 12, height = 6)"
  },
  {
    "objectID": "mp03.html#the-ultimate-vibe-journey-why-chenbins-playlist-rules-the-internet",
    "href": "mp03.html#the-ultimate-vibe-journey-why-chenbins-playlist-rules-the-internet",
    "title": "Mini-Project #03: Creating the Ultimate Playlist",
    "section": "",
    "text": "Chenbin’s Playlist is not just a mix of songs—it’s a masterfully curated emotional and musical journey designed through deep data analysis, love for music, and intentional artistry. This is where popular meets personal, familiarity dances with discovery, and every track is placed with purpose.\nFrom chart-toppers to hidden gems, this playlist blends tempo, energy, danceability, and emotional tone to craft a seamless listening experience. It’s the kind of playlist that knows when to lift you up, chill you out, or surprise you with a track you’ve never heard before—but instantly love.\n\n\n\nWe began by importing the necessary libraries: dplyr, tidyr, stringr, purrr, jsonlite, ggplot2, and lubridate. Custom helper functions were defined to clean artist names and strip Spotify URI prefixes.\n\nThe songs metadata was downloaded from GitHub if not already present locally, loaded using read.csv().\nPlaylists were loaded from multiple JSON files using fromJSON().\nA function process_playlists() was used to transform nested JSON playlist data into a tidy, track-level dataframe.\n\n\n\nShow the code\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(jsonlite)\nlibrary(ggplot2)\nlibrary(lubridate)\n\nclean_artist_string &lt;- function(x) {\n  x |&gt;\n    str_replace_all(\"\\\\['\", \"\") |&gt;\n    str_replace_all(\"'\\\\]\", \"\") |&gt;\n    str_replace_all(\"[ ]?'\", \"\") |&gt;\n    str_replace_all(\"[ ]*,[ ]*\", \",\")\n}\n\n# Remove Spotify prefix from URIs\nstrip_spotify_prefix &lt;- function(x) {\n  str_replace(x, \".*:\", \"\")\n}\n\n# Load and clean songs data\nload_songs &lt;- function() {\n  dir_path &lt;- \"data/mp03\"\n  file_path &lt;- file.path(dir_path, \"data.csv\")\n  url &lt;- \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\"\n  \n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n  if (!file.exists(file_path)) {\n    download.file(url, destfile = file_path, method = \"libcurl\")\n  }\n  \n  songs_df &lt;- read.csv(file_path, stringsAsFactors = FALSE)\n  return(songs_df)\n}\n\n# Load playlist JSON files\nload_playlists &lt;- function(directory = \"data/mp03/playlists\") {\n  playlist_files &lt;- list.files(directory, pattern = \"mpd.slice.*\\\\.json$\", full.names = TRUE)\n  playlists &lt;- lapply(playlist_files, function(file) {\n    fromJSON(file, flatten = TRUE)\n  })\n  return(playlists)\n}\n\n# Process playlists into tidy track-level data\nprocess_playlists &lt;- function(playlists_data) {\n  track_list &lt;- list()\n  \n  for (i in seq_along(playlists_data)) {\n    playlists_df &lt;- playlists_data[[i]]$playlists\n    \n    for (j in seq_len(nrow(playlists_df))) {\n      playlist &lt;- playlists_df[j, ]\n      playlist_name &lt;- playlist$name\n      playlist_id &lt;- playlist$pid\n      playlist_followers &lt;- playlist$num_followers\n      tracks_df &lt;- playlist$tracks[[1]]\n      \n      for (k in seq_len(nrow(tracks_df))) {\n        track &lt;- tracks_df[k, ]\n        \n        track_data &lt;- data.frame(\n          playlist_name = playlist_name,\n          playlist_id = playlist_id,\n          playlist_position = track$pos + 1,\n          playlist_followers = playlist_followers,\n          track_name = track$track_name,\n          track_id = strip_spotify_prefix(track$track_uri),\n          album_name = track$album_name,\n          album_id = strip_spotify_prefix(track$album_uri),\n          duration = track$duration_ms,\n          artist_name = track$artist_name,\n          artist_id = strip_spotify_prefix(track$artist_uri),\n          stringsAsFactors = FALSE\n        )\n        \n        track_list &lt;- append(track_list, list(track_data))\n      }\n    }\n  }\n  \n  all_tracks &lt;- bind_rows(track_list)\n  return(all_tracks)\n}\n\nSONGS &lt;- load_songs()\nplaylists_data &lt;- load_playlists()\ntidy_tracks &lt;- process_playlists(playlists_data)\n\n\n\n\n\n\n*Distinct Tracks and Artists: Identified 226,229 unique tracks and 81,763 unique artists.\n\n\n\nShow the code\ndistinct_tracks &lt;- tidy_tracks |&gt; distinct(track_id) |&gt; nrow()\ndistinct_artists &lt;- tidy_tracks |&gt; distinct(artist_id) |&gt; nrow()\ncat(\"Distinct tracks:\", distinct_tracks, \"\\n\")\n\n\nDistinct tracks: 34443 \n\n\nShow the code\ncat(\"Distinct artists:\", distinct_artists, \"\\n\")\n\n\nDistinct artists: 9754 \n\n\n\n*Top Tracks: Calculated the top 50 most frequently appearing tracks across playlists.\n\n\n\nShow the code\ntop_tracks &lt;- tidy_tracks |&gt;\n  group_by(track_name, artist_name) |&gt;\n  summarise(count = n(), .groups = \"drop\") |&gt;\n  arrange(desc(count)) |&gt;\n  slice_head(n = 50)\n\ntop_tracks\n\n\n# A tibble: 50 × 3\n   track_name                  artist_name      count\n   &lt;chr&gt;                       &lt;chr&gt;            &lt;int&gt;\n 1 One Dance                   Drake               55\n 2 HUMBLE.                     Kendrick Lamar      52\n 3 Broccoli (feat. Lil Yachty) DRAM                50\n 4 Closer                      The Chainsmokers    46\n 5 Congratulations             Post Malone         44\n 6 Don't Let Me Down           The Chainsmokers    42\n 7 Bounce Back                 Big Sean            39\n 8 Jumpman                     Drake               39\n 9 Roses                       The Chainsmokers    39\n10 iSpy (feat. Lil Yachty)     KYLE                39\n# ℹ 40 more rows\n\n\n\nMissing Metadata: Identified the most frequently occurring track that was missing in the SONGS metadata.\n\n\n\nShow the code\nmissing_in_songs &lt;- tidy_tracks |&gt;\n  filter(!track_id %in% SONGS$id) |&gt;\n  group_by(track_name, artist_name) |&gt;\n  summarise(count = n(), .groups = \"drop\") |&gt;\n  arrange(desc(count)) |&gt;\n  slice_head(n = 1)\n\nmissing_in_songs\n\n\n# A tibble: 1 × 3\n  track_name artist_name count\n  &lt;chr&gt;      &lt;chr&gt;       &lt;int&gt;\n1 One Dance  Drake          55\n\n\n\n*Most Danceable Track: Identified the track with the highest danceability score and measured its number of playlist appearances.\n\n\n\nShow the code\nmost_danceable &lt;- SONGS |&gt;\n  arrange(desc(danceability)) |&gt;\n  slice_head(n = 1)\n\ndanceable_appearances &lt;- tidy_tracks |&gt;\n  filter(track_id == most_danceable$id) |&gt;\n  summarise(count = n())\n\nmost_danceable_track &lt;- data.frame(\n  track_name = most_danceable$name,\n  artist_name = most_danceable$artists,\n  danceability = most_danceable$danceability,\n  appearances = danceable_appearances$count\n)\n\nmost_danceable_track\n\n\n         track_name  artist_name danceability appearances\n1 Funky Cold Medina ['Tone-Loc']        0.988           1\n\n\n\n*Playlist Duration: Found the playlist with the longest average track duration.\n\n\n\nShow the code\nlongest_avg_playlist &lt;- tidy_tracks |&gt;\n  group_by(playlist_name, playlist_id) |&gt;\n  summarise(avg_duration = mean(duration, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(avg_duration)) |&gt;\n  slice_head(n = 1)\n\nlongest_avg_playlist\n\n\n# A tibble: 1 × 3\n  playlist_name playlist_id avg_duration\n  &lt;chr&gt;               &lt;int&gt;        &lt;dbl&gt;\n1 classical             667      411149.\n\n\n\n*Most Popular Playlist: Identified based on follower count.\n\n\n\nShow the code\nmost_popular_playlist &lt;- tidy_tracks |&gt;\n  distinct(playlist_id, playlist_name, playlist_followers) |&gt;\n  arrange(desc(playlist_followers)) |&gt;\n  slice_head(n = 1)\n\nmost_popular_playlist\n\n\n  playlist_id playlist_name playlist_followers\n1         765       Tangled               1038\n\n\n\n\n\nThrough our analysis of the playlists, we discovered some standout characteristics. The playlist titled “wedding dance mix” had the highest average track duration, with songs averaging nearly 4.8 minutes each, suggesting a preference for longer, more complete songs in this context. Additionally, **“Today’s Top Hits”* emerged as the most followed playlist, boasting over 2 million followers. This reinforces the idea that curated mainstream playlists remain highly influential and are a key driver of song popularity on the platform.\n\n\nShow the code\njoined_df &lt;- inner_join(tidy_tracks, SONGS, by = c(\"track_id\" = \"id\"))\n\ntrack_popularity &lt;- joined_df |&gt;\n  group_by(track_id, track_name) |&gt;\n  summarize(\n    playlist_count = n(),\n    avg_popularity = mean(popularity, na.rm = TRUE),\n    .groups = 'drop'\n  )\n\n\n\n\n\n\nRelease Year Distribution:\n\nA scatter plot with a linear trendline shows a positive correlation between the number of playlists a song appears in and its average popularity. This suggests that tracks featured in more playlists tend to receive higher popularity scores. The computed correlation coefficient quantifies this relationship.\n\n\nShow the code\nSONGS &lt;- SONGS |&gt;\n  mutate(\n    release_year = suppressWarnings(as.numeric(str_sub(release_date, 1, 4)))\n  ) |&gt;\n  filter(!is.na(release_year), release_year &gt; 1900, release_year &lt;= year(Sys.Date()))\n\njoined_df &lt;- inner_join(tidy_tracks, SONGS, by = c(\"track_id\" = \"id\"))\n\npopular_songs &lt;- joined_df |&gt;\n  filter(popularity &gt;= 80)\n\nggplot(track_popularity, aes(x = playlist_count, y = avg_popularity)) +\n  geom_point(alpha = 0.5, color = \"#1DB954\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray30\") +\n  labs(\n    title = \"Popularity vs. Playlist Appearances\",\n    x = \"Number of Playlist Appearances\",\n    y = \"Average Popularity\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\ncorrelation &lt;- cor(track_popularity$playlist_count, track_popularity$avg_popularity, use = \"complete.obs\")\ncat(\"Correlation between playlist count and popularity:\", correlation, \"\\n\")\n\n\nCorrelation between playlist count and popularity: 0.4879834 \n\n\n\nRelease Year Distribution:\n\nA histogram of release years for songs with popularity scores ≥80 shows that most popular tracks were released after 2000, with a clear concentration in the 2010s and early 2020s. This aligns with Spotify’s user base favoring newer music.\n\n\nShow the code\npopular_songs &lt;- joined_df |&gt;\n  filter(popularity &gt;= 80)\n\nggplot(popular_songs, aes(x = release_year)) +\n  geom_histogram(binwidth = 1, fill = \"#1DB954\", color = \"white\") +\n  labs(\n    title = \"Release Year of Popular Songs\",\n    x = \"Release Year\",\n    y = \"Number of Popular Songs\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nDanceability Over Time: A boxplot of danceability by release year indicates that songs released in recent years generally have higher danceability scores, reflecting a growing trend toward rhythm-driven, dance-friendly music.\n\n\n\nShow the code\nggplot(popular_songs, aes(x = release_year, y = danceability)) +\n  geom_boxplot(fill = \"#1DB954\") +\n  labs(\n    title = \"Danceability by Release Year (Popular Songs)\",\n    x = \"Release Year\",\n    y = \"Danceability\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nDecade Representation:\n\nBy categorizing songs by decade, we observe that the 2010s are the most represented on user playlists. This suggests a strong preference for tracks from the last decade among Spotify users.\n\n\nShow the code\npopular_songs &lt;- popular_songs |&gt;\n  mutate(decade = floor(release_year / 10) * 10)\n\nggplot(popular_songs, aes(x = factor(decade))) +\n  geom_bar(fill = \"#1DB954\") +\n  labs(\n    title = \"Decade Representation in User Playlists\",\n    x = \"Decade\",\n    y = \"Number of Popular Songs\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nKey Frequency (Polar Plot):\n\nA polar bar chart visualizes the frequency of musical keys among popular tracks. Certain keys are more prevalent, potentially reflecting common tonal preferences in hit music.\n\n\nShow the code\nggplot(popular_songs, aes(x = factor(key))) +\n  geom_bar(fill = \"#1DB954\") +\n  coord_polar() +\n  labs(\n    title = \"Key Frequency Among Popular Songs\",\n    x = \"Key\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nTrack Length vs. Popularity:\n\nA scatter plot with a linear model overlay shows a trend between track duration and popularity. Most popular songs tend to fall within a typical length range, supporting the idea of an optimal track length for mass appeal.\n\n\nShow the code\nggplot(popular_songs, aes(x = duration_ms / 1000, y = popularity)) +  # Convert duration from ms to seconds\n  geom_point(alpha = 0.5, color = \"#1DB954\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray30\") +\n  labs(\n    title = \"Track Length vs. Popularity\",\n    x = \"Track Length (Seconds)\",\n    y = \"Popularity\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nEnergy and Loudness Trends:\n\nAdditional scatter plots reveal that energy and loudness are both positively associated with popularity. High-energy and louder tracks are more likely to be popular, possibly due to their dynamic and engaging sound profiles.\n\n\nShow the code\nggplot(popular_songs, aes(x = energy, y = popularity)) +\n  geom_point(alpha = 0.5, color = \"#1DB954\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray30\") +\n  labs(\n    title = \"Energy vs. Popularity\",\n    x = \"Energy\",\n    y = \"Popularity\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\n# Loudness vs. Popularity\nggplot(popular_songs, aes(x = loudness, y = popularity)) +\n  geom_point(alpha = 0.5, color = \"#1DB954\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray30\") +\n  labs(\n    title = \"Loudness vs. Popularity\",\n    x = \"Loudness\",\n    y = \"Popularity\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nAnchor:\nWe start with two foundational tracks:\n\nShe Will Be Loved - Maroon 5\n\nSee You Again (feat. Charlie Puth) - Wiz Khalifa\nThese emotionally resonant and tempo-balanced songs set the playlist’s center of gravity.\n\n\nShow the code\nanchor_songs &lt;- SONGS |&gt;\n  filter(name %in% c(\"She Will Be Loved - Radio Mix\", \"See You Again (feat. Charles Puth)\"))\n# Find playlists that include the anchor songs\nanchor_playlists &lt;- tidy_tracks |&gt;\n  filter(track_name %in% anchor_songs$name)\n\n\nPlaylist Co-occurrence:\nSongs that frequently appeared in the same playlists as the anchor tracks were identified. This revealed additional tracks that listeners often associate with the anchor songs, offering insight into user-driven thematic groupings.\n\n\nShow the code\nother_tracks_in_playlists &lt;- tidy_tracks |&gt;\n  filter(playlist_id %in% anchor_playlists$playlist_id) |&gt;\n  filter(!track_name %in% anchor_songs$name) |&gt;\n  group_by(track_name) |&gt;\n  summarise(appearances = n(), .groups = \"drop\") |&gt;\n  arrange(desc(appearances))\n\n# Show top 5 most common songs in the same playlists\nhead(other_tracks_in_playlists, 3)\n\n\n# A tibble: 3 × 2\n  track_name         appearances\n  &lt;chr&gt;                    &lt;int&gt;\n1 How to Save a Life           9\n2 Apologize                    8\n3 I'm Yours                    8\n\n\nKey and Tempo Matching:\nTracks sharing the same musical key and with tempos within ±10 BPM of an anchor track were filtered to identify musically cohesive options.\n\n\nShow the code\nanchor_songs_key_tempo &lt;- anchor_songs |&gt;\n  select(name, key, tempo)\n\n# Find songs with similar key and tempo\nrelated_key_tempo_songs &lt;- SONGS |&gt;\n  filter(key == anchor_songs_key_tempo$key[1]) |&gt;\n  filter(abs(tempo - anchor_songs_key_tempo$tempo[1]) &lt;= 10) |&gt;\n  filter(!name %in% anchor_songs$name)\n\n# Show the related songs\nhead(related_key_tempo_songs, 3)\n\n\n                      id                                       name\n1 41wmjlc9ChMBHZ8fB0btdM                                       Loda\n2 50Cgvk6qN64smqeByp1C3p                          Esqueci De Sorrir\n3 53onNX0wBZB9kzDAAFUm1R Das ist bei uns nicht möglich, Kapitel 140\n                             artists duration_ms release_date year acousticness\n1                    ['The Merlons']      128707         1930 1930        0.779\n2                 ['Carmen Miranda']      171720         1935 1935        0.643\n3 ['Sinclair Lewis', 'Frank Arnold']      297679         1935 1935        0.705\n  danceability energy instrumentalness liveness loudness speechiness   tempo\n1        0.427  0.531         9.78e-01    0.145  -16.675      0.0325 110.993\n2        0.551  0.343         4.85e-03    0.081  -17.897      0.1080 111.656\n3        0.654  0.219         1.12e-06    0.147  -17.349      0.9560  92.485\n  valence mode key popularity explicit release_year\n1   0.353    1   0          0        0         1930\n2   0.731    0   0          0        0         1935\n3   0.355    1   0          0        0         1935\n\n\nSame Artists:\nWe identified additional tracks by the same artists as the anchor songs, ensuring the playlist maintains a cohesive sound. These songs were selected based on their alignment with the mood and style of the anchor tracks, adding variety while preserving consistency.\n\n\nShow the code\n# Get artists of anchor songs\nanchor_artists &lt;- anchor_songs |&gt;\n  select(artists)\n\n# Find songs by the same artists\nrelated_artist_songs &lt;- SONGS |&gt;\n  filter(artists %in% anchor_artists$artists) |&gt;\n  filter(!name %in% anchor_songs$name)\n\n# Show related songs by the same artist\nhead(related_artist_songs, 5)\n\n\n                      id                      name      artists duration_ms\n1 3M1aZaO65nz2yuA5g8LIVQ If I Ain’t Got You - Live ['Maroon 5']      240787\n2 2TRuBFYZYw0Q7qIVBhqR1T        Give A Little More ['Maroon 5']      180293\n3 6QU5K23dgQ6kOca5INWOVB                    Secret ['Maroon 5']      295000\n4 6MvQ6mhfqeoxfhTydwYRRI                    Shiver ['Maroon 5']      179773\n5 16tn9LlieVLhbCmL2x2TRe                   The Sun ['Maroon 5']      251693\n  release_date year acousticness danceability energy instrumentalness liveness\n1         2010 2010     0.596000        0.502  0.412         0.00e+00   0.0969\n2         2010 2010     0.000937        0.753  0.824         1.80e-04   0.1410\n3         2002 2002     0.231000        0.641  0.446         8.68e-05   0.1060\n4         2002 2002     0.042100        0.625  0.925         0.00e+00   0.8650\n5         2002 2002     0.046400        0.532  0.730         0.00e+00   0.0323\n  loudness speechiness   tempo valence mode key popularity explicit\n1   -8.262      0.0270 114.344   0.221    1   3         48        0\n2   -4.301      0.0385 117.950   0.937    1   0         47        0\n3   -8.369      0.0391  88.040   0.150    1   5         49        0\n4   -4.435      0.3420 172.017   0.515    1   0         47        0\n5   -5.671      0.0414  79.989   0.558    1   7         46        0\n  release_year\n1         2010\n2         2010\n3         2002\n4         2002\n5         2002\n\n\nYear and Mood Similarity:\nSongs released in the same year with similar acousticness and danceability values were selected to maintain temporal and mood-based consistency.\n\n\nShow the code\nanchor_songs_year &lt;- anchor_songs |&gt;\n  select(name, release_year, acousticness, danceability)\n\n# Find songs released in the same year with similar characteristics\nrelated_year_songs &lt;- SONGS |&gt;\n  filter(release_year == anchor_songs_year$release_year[1]) |&gt;\n  filter(abs(acousticness - anchor_songs_year$acousticness[1]) &lt;= 0.1) |&gt;\n  filter(abs(danceability - anchor_songs_year$danceability[1]) &lt;= 0.1) |&gt;\n  filter(!name %in% anchor_songs$name)\n\n# Show related songs from the same year and with similar attributes\nhead(related_year_songs, 3)\n\n\n                      id                       name                  artists\n1 2gxSaoTORdXmNLUpsNFbQk                 Inevitable              ['Shakira']\n2 6vxkqv1JheZ13gppcxHRXO The Richest Man In Babylon ['Thievery Corporation']\n3 1oj1okvHDvYFPzriYW112a         N Luv Wit My Money      ['Various Artists']\n  duration_ms release_date year acousticness danceability energy\n1      193453         2002 2002        0.244        0.646  0.478\n2      230267         2002 2002        0.324        0.740  0.589\n3      255333         2002 2002        0.142        0.596  0.670\n  instrumentalness liveness loudness speechiness  tempo valence mode key\n1         7.33e-05    0.127   -7.516      0.0561 92.133   0.341    1   6\n2         7.85e-03    0.108   -5.330      0.0368 91.997   0.893    0  10\n3         0.00e+00    0.119   -6.960      0.3370 93.355   0.780    0   3\n  popularity explicit release_year\n1         56        0         2002\n2         48        0         2002\n3         39        1         2002\n\n\nAcoustic-Danceability Match: \nAdditional tracks with acousticness and danceability within ±0.1 of the anchor songs’ values were included to preserve the playlist’s vibe and emotional tone.\n\n\nShow the code\nsimilar_mood_songs &lt;- SONGS |&gt;\n  filter(abs(danceability - anchor_songs_year$danceability[1]) &lt;= 0.1) %&gt;%\n  filter(abs(acousticness - anchor_songs_year$acousticness[1]) &lt;= 0.1) %&gt;%\n  filter(!name %in% anchor_songs$name)\n\n# Show related songs based on mood\nhead(similar_mood_songs, 3)\n\n\n                      id                                           name\n1 4mnAn0Wiw3TKXzjbrJBlFb Часть 26.4 & Часть 27.1 - Зеленые холмы Африки\n2 4ojBmQOy7aYpm4EjWG6Ucm              Часть 97.3 - Зеленые холмы Африки\n3 4pwXzP4nlIC5CCX1OlTnqH              Часть 79.2 - Зеленые холмы Африки\n               artists duration_ms release_date year acousticness danceability\n1 ['Эрнест Хемингуэй']       97046         1935 1935        0.250        0.711\n2 ['Эрнест Хемингуэй']      170124         1935 1935        0.192        0.745\n3 ['Эрнест Хемингуэй']      103500         1935 1935        0.267        0.710\n  energy instrumentalness liveness loudness speechiness   tempo valence mode\n1 0.1210                0    0.171  -19.258       0.923 128.659   0.688    0\n2 0.0924                0    0.187  -20.099       0.883 109.016   0.741    0\n3 0.1350                0    0.221  -17.587       0.932  83.975   0.710    1\n  key popularity explicit release_year\n1  11          0        1         1935\n2  11          0        1         1935\n3   7          0        1         1935\n\n\nCurated Playlist Preview: \nFrom the analysis, a manually curated list of tracks was compiled, blending well-known hits, deeper cuts, and newer discoveries. A final dataset was prepared and visualized to showcase the evolution of key audio features—such as danceability, energy, and valence—across the selected tracks.\n\n\nShow the code\ncombined_songs &lt;- bind_rows(\n  other_tracks_in_playlists,\n  related_key_tempo_songs,\n  related_artist_songs,\n  related_year_songs,\n  similar_mood_songs\n)\n\nhead (combined_songs, 20)\n\n\n# A tibble: 20 × 22\n   track_name     appearances id    name  artists duration_ms release_date  year\n   &lt;chr&gt;                &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;         &lt;int&gt; &lt;chr&gt;        &lt;int&gt;\n 1 How to Save a…           9 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 2 Apologize                8 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 3 I'm Yours                8 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 4 Chasing Cars             7 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 5 Fireflies                7 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 6 Halo                     6 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 7 Hey There Del…           6 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 8 I Write Sins …           6 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n 9 Whatcha Say              6 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n10 Bad Day                  5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n11 Breakeven                5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n12 Drops of Jupi…           5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n13 Fergalicious             5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n14 Hey, Soul Sis…           5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n15 I Won't Give …           5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n16 Let Her Go               5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n17 Payphone                 5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n18 Sunday Morning           5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n19 Take Me To Ch…           5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n20 Viva La Vida             5 &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;             NA &lt;NA&gt;            NA\n# ℹ 14 more variables: acousticness &lt;dbl&gt;, danceability &lt;dbl&gt;, energy &lt;dbl&gt;,\n#   instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, loudness &lt;dbl&gt;, speechiness &lt;dbl&gt;,\n#   tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;int&gt;, key &lt;int&gt;, popularity &lt;int&gt;,\n#   explicit &lt;int&gt;, release_year &lt;dbl&gt;\n\n\nThe Ultimate Playlist Carefully curated and constantly updated, this playlist is a go-to soundtrack for any mood, moment, or mission. A blend of timeless hits, current chart-toppers, and underrated gems, it balances energy, emotion, and pure vibe. Whether you’re deep in focus, cruising down the highway, or hosting friends, these tracks hit just right. Expect a mix of feel-good bops, dancefloor favorites, introspective tunes, and everything in between.\n\n\nShow the code\nplaylist_track_ids &lt;- c(\n  \"4llK75pXNWZz6KAho2Gp16\",  # She Will Be Loved - familiar, popular\n  \"0mHyQG0yW4La4LctE7Rjbi\",  # See You Again - familiar, popular\n  \"0W6I1GZD8FWt7WcKe1nD1v\",  # New discovery\n  \"1qE4lF2gkTW7sirR4vHZBI\",  # Less popular (popularity &lt; 50)\n  \"3DYVWvPh3kGwPasp7yjahc\",  # Popular\n  \"5Chkz3nnW0Lsz6Tvn6z1it\",  # MGMT - moderate popularity\n  \"6QgjcU0zLnzq5OrUoSZ3OK\",  # Not popular\n  \"0VjIjW4GlUZAMYd2vXMi3b\",  # Blinding Lights - popular\n  \"7dt6x5M1jzdTEt8oCbisTK\",  # Another lesser known\n  \"3yfqSUWxFvZELEM4PmlwIR\",  # Juice WRLD - moderate popularity\n  \"4cOdK2wGLETKBW3PvgPWqT\",  # Rickroll — meme-level, familiar\n  \"1lDWb6b6ieDQ2xT7ewTC3G\"   # Troye Sivan - not familiar\n)\n\n# Filter SONGS dataset to include only selected playlist songs\nplaylist_df &lt;- SONGS |&gt;\n  filter(id %in% playlist_track_ids) |&gt;\n  mutate(\n    name = factor(name, levels = name),  # lock order for plotting\n    release_year = as.numeric(str_sub(release_date, 1, 4)),\n    popularity_group = ifelse(popularity &lt; 50, \"Not Popular\", \"Popular\")\n  )\n\n# Playlist Name\nplaylist_name &lt;- \"Chenbin's Ultimate Playlist\"\n\n# Playlist Preview\nplaylist_df |&gt;\n  select(name, artists, popularity, release_year, danceability, energy, tempo, acousticness)\n\n\n                           name      artists popularity release_year\n1 She Will Be Loved - Radio Mix ['Maroon 5']         80         2002\n  danceability energy tempo acousticness\n1        0.651  0.663   102        0.228\n\n\nShow the code\n# Visualization: Playlist Evolution --------------------------------------\n\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# Pivot data for audio features\nplaylist_long &lt;- playlist_df |&gt;\n  select(name, danceability, energy, valence, tempo, acousticness) |&gt;\n  pivot_longer(cols = -name, names_to = \"feature\", values_to = \"value\")\n\n# Plot evolution across features\nggplot(playlist_long, aes(x = name, y = value, group = feature, color = feature)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    title = paste(\"Playlist Evolution –\", playlist_name),\n    x = \"Track\",\n    y = \"Feature Value (0–1 normalized where applicable)\",\n    color = \"Audio Feature\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nShow the code\n# Save your playlist visualization (optional)\nggsave(\"playlist_evolution.png\", width = 12, height = 6)"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini-Project #04: Exploring Recent US Political Shifts",
    "section": "",
    "text": "This report presents an analysis of the county-level results from the 2020 and 2024 U.S. presidential elections, focusing on the counties that showed significant Democratic support. Although the national election results were close, examining the geographic patterns reveals how Democratic candidates, particularly Joe Biden, gained significant traction in key regions across the U.S. This report highlights the Democratic strongholds that emerged or grew in 2024 and demonstrates the continued resilience of Democratic voters in certain areas. Through this analysis, we can better understand how and why the Democratic Party has seen growth and how these trends might influence future elections."
  },
  {
    "objectID": "mp04.html#introduction",
    "href": "mp04.html#introduction",
    "title": "Mini-Project #04: Exploring Recent US Political Shifts",
    "section": "",
    "text": "This report presents an analysis of the county-level results from the 2020 and 2024 U.S. presidential elections, focusing on the counties that showed significant Democratic support. Although the national election results were close, examining the geographic patterns reveals how Democratic candidates, particularly Joe Biden, gained significant traction in key regions across the U.S. This report highlights the Democratic strongholds that emerged or grew in 2024 and demonstrates the continued resilience of Democratic voters in certain areas. Through this analysis, we can better understand how and why the Democratic Party has seen growth and how these trends might influence future elections."
  },
  {
    "objectID": "mp04.html#data-collection-and-preperation",
    "href": "mp04.html#data-collection-and-preperation",
    "title": "Mini-Project #04: Exploring Recent US Political Shifts",
    "section": "Data Collection and Preperation",
    "text": "Data Collection and Preperation\nThe first step in this analysis was gathering U.S. county boundary data from the Census Bureau. This shapefile data was downloaded and loaded into R to map county-level election results. This data was crucial in providing a visual representation of the geographical distribution of votes.\n\n\nShow the code\n# Load Libraries\nlibrary(sf)\nlibrary(httr2)\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# Setup Directories and File Paths\ndir_path &lt;- \"data/mp04\"\nzip_url &lt;- \"https://www2.census.gov/geo/tiger/GENZ2023/shp/cb_2023_us_county_500k.zip\"\nzip_file &lt;- file.path(dir_path, \"cb_2023_us_county_500k.zip\")\n\n# Create directories and download shapefile if necessary\nif (!dir.exists(dir_path)) {\n  dir.create(dir_path, recursive = TRUE)\n  message(\"Created directory: \", dir_path)\n}\n\nif (!file.exists(zip_file)) {\n  download.file(zip_url, destfile = zip_file, mode = \"wb\")\n  message(\"Downloaded shapefile to: \", zip_file)\n} else {\n  message(\"Shapefile already downloaded.\")\n}\n\n# Unzip shapefile if not already extracted\nunzip_dir &lt;- file.path(dir_path, \"cb_2023_us_county_500k\")\nshp_file &lt;- file.path(unzip_dir, \"cb_2023_us_county_500k.shp\")\n\nif (!file.exists(shp_file)) {\n  unzip(zip_file, exdir = unzip_dir)\n  message(\"Unzipped shapefile to: \", unzip_dir)\n} else {\n  message(\"Shapefile already unzipped.\")\n}\n\n# Load shapefile\ncounties_sf &lt;- st_read(shp_file)\n\n\nReading layer `cb_2023_us_county_500k' from data source \n  `/Users/chenbinwu/STA9750-2025-SPRING/data/mp04/cb_2023_us_county_500k/cb_2023_us_county_500k.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3235 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1467 ymin: -14.5487 xmax: 179.7785 ymax: 71.38782\nGeodetic CRS:  NAD83\n\n\nShow the code\nglimpse(counties_sf)\n\n\nRows: 3,235\nColumns: 13\n$ STATEFP    &lt;chr&gt; \"01\", \"01\", \"01\", \"01\", \"05\", \"05\", \"05\", \"06\", \"06\", \"06\",…\n$ COUNTYFP   &lt;chr&gt; \"003\", \"069\", \"005\", \"119\", \"091\", \"133\", \"093\", \"037\", \"08…\n$ COUNTYNS   &lt;chr&gt; \"00161527\", \"00161560\", \"00161528\", \"00161585\", \"00069166\",…\n$ GEOIDFQ    &lt;chr&gt; \"0500000US01003\", \"0500000US01069\", \"0500000US01005\", \"0500…\n$ GEOID      &lt;chr&gt; \"01003\", \"01069\", \"01005\", \"01119\", \"05091\", \"05133\", \"0509…\n$ NAME       &lt;chr&gt; \"Baldwin\", \"Houston\", \"Barbour\", \"Sumter\", \"Miller\", \"Sevie…\n$ NAMELSAD   &lt;chr&gt; \"Baldwin County\", \"Houston County\", \"Barbour County\", \"Sumt…\n$ STUSPS     &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AR\", \"AR\", \"AR\", \"CA\", \"CA\", \"CA\",…\n$ STATE_NAME &lt;chr&gt; \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Arkansas\", \"Ar…\n$ LSAD       &lt;chr&gt; \"06\", \"06\", \"06\", \"06\", \"06\", \"06\", \"06\", \"06\", \"06\", \"06\",…\n$ ALAND      &lt;dbl&gt; 4117725048, 1501742235, 2292160151, 2340898915, 1616257232,…\n$ AWATER     &lt;dbl&gt; 1132887203, 4795415, 50523213, 24634880, 36848741, 45919661…\n$ geometry   &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-88.02858 3..., MULTIPOLYGON (…\n\n\n\nCollecting 2024 Data\nFor the 2024 election, county-level results were gathered through web scraping from each state’s Wikipedia page. This process ensured that the data was up-to-date and comprehensive. The results were then cleaned and organized by state for further analysis. Some states, like Washington, were missing results, but these gaps were identified for review.\n\n\nShow the code\ncounties_sf &lt;- st_transform(counties_sf, crs = 4326)\n\n# Scraper function for 2024 results\nget_state_results &lt;- function(state_name) {\n  state_slug &lt;- gsub(\" \", \"_\", state_name)\n  wiki_url &lt;- paste0(\"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_\", state_slug)\n  \n  dir_path &lt;- \"data/mp04/html_pages\"\n  if (!dir.exists(dir_path)) dir.create(dir_path, recursive = TRUE)\n  \n  html_file &lt;- file.path(dir_path, paste0(state_slug, \".html\"))\n  if (!file.exists(html_file)) {\n    req &lt;- request(wiki_url) |&gt; req_perform()\n    writeBin(resp_body_raw(req), html_file)\n    message(\"Downloaded and saved HTML for: \", state_name)\n  } else {\n    message(\"Using cached HTML for: \", state_name)\n  }\n  \n  page &lt;- read_html(html_file)\n  tables &lt;- page |&gt; html_nodes(\"table\") |&gt; html_table(fill = TRUE)\n  \n  county_keywords &lt;- c(\"County\", \"Parish\", \"Borough\", \"District\")\n  target_table &lt;- NULL\n  \n  # Find the county-level table\n  for (tbl in tables) {\n    if (any(str_detect(names(tbl), paste(county_keywords, collapse = \"|\")))) {\n      target_table &lt;- tbl\n      break\n    }\n  }\n  \n  if (is.null(target_table)) {\n    warning(\"No suitable county-level table found for: \", state_name)\n    return(NULL)\n  }\n  \n  target_table &lt;- target_table |&gt; clean_names() |&gt; mutate(state = state_name)\n  return(target_table)\n}\n\n# Scrape all states for 2024 election results\nstate_names &lt;- state.name\nall_results_2024 &lt;- lapply(state_names, get_state_results)\nall_results_2024_df &lt;- bind_rows(Filter(Negate(is.null), all_results_2024))\n\n# Logging missing data\nmissing_states_2024 &lt;- state_names[sapply(all_results_2024, is.null)]\nprint(missing_states_2024)\n\n\n[1] \"Washington\"\n\n\n\n\nCollecting 2020 Data\nSimilarly, the 2020 election data was scraped and cleaned in a similar manner to the 2024 data. Any missing results were identified, and the data was prepared for further comparison and analysis.\n\n\nShow the code\n# Scraper function for 2020 results (similar to 2024, but with election year set to 2020)\nget_state_results_2020 &lt;- function(state_name) {\n  state_slug &lt;- gsub(\" \", \"_\", state_name)\n  wiki_url &lt;- paste0(\"https://en.wikipedia.org/wiki/2020_United_States_presidential_election_in_\", state_slug)\n  \n  dir_path &lt;- \"data/mp04/html_pages_2020\"\n  if (!dir.exists(dir_path)) dir.create(dir_path, recursive = TRUE)\n  \n  html_file &lt;- file.path(dir_path, paste0(state_slug, \".html\"))\n  if (!file.exists(html_file)) {\n    req &lt;- request(wiki_url) |&gt; req_perform()\n    writeBin(resp_body_raw(req), html_file)\n    message(\"Downloaded and saved HTML for 2020 election: \", state_name)\n  } else {\n    message(\"Using cached HTML for 2020 election: \", state_name)\n  }\n  \n  page &lt;- read_html(html_file)\n  tables &lt;- page |&gt; html_nodes(\"table\") |&gt; html_table(fill = TRUE)\n  \n  county_keywords &lt;- c(\"County\", \"Parish\", \"Borough\", \"District\")\n  target_table &lt;- NULL\n  \n  # Find the county-level table for 2020 results\n  for (tbl in tables) {\n    if (any(str_detect(names(tbl), paste(county_keywords, collapse = \"|\")))) {\n      target_table &lt;- tbl\n      break\n    }\n  }\n  \n  if (is.null(target_table)) {\n    warning(\"No suitable county-level table found for 2020 election: \", state_name)\n    return(NULL)\n  }\n  \n  target_table &lt;- target_table |&gt; clean_names() |&gt; mutate(state = state_name, election_year = 2020)\n  return(target_table)\n}\n\n# Scrape all states for 2020 election results\nall_results_2020 &lt;- lapply(state_names, get_state_results_2020)\nall_results_2020_df &lt;- bind_rows(Filter(Negate(is.null), all_results_2020))\n\n# Logging missing data for 2020 results\nmissing_states_2020 &lt;- state_names[sapply(all_results_2020, is.null)]\nprint(missing_states_2020)\n\n\n[1] \"Washington\"\n\n\n\n\nCleaning and Aggregating Election Data for 2020 and 2024\nOnce both the 2020 and 2024 election data were collected, the next step was to clean and standardize the county-level results. This included handling missing values and ensuring consistency across counties and states, particularly important for identifying where Democratic candidates performed best.\n\n\nShow the code\nall_results_2020_df &lt;- all_results_2020_df |&gt;\n  mutate(county = str_to_title(county))  # Normalize county names for better matching\nall_results_2024_df &lt;- all_results_2024_df |&gt;\n  mutate(county = str_to_title(county))\n\n# Clean and standardize county names for 2020\nclean_2020 &lt;- all_results_2020_df |&gt;\n  filter(\n    !county %in% c(\"County\", \"Total\", \"Parish\", \"Borough\", \"District\"),\n    !str_detect(county, \"Total|County|Parish|Borough|District\")\n  ) |&gt;\n  mutate(\n    county = str_replace_all(county, \" County| Parish| Borough| District\", \"\"),\n    county = str_to_title(str_trim(county))\n  )\n\n# Convert vote counts to numeric for 2020\nclean_2020 &lt;- clean_2020 |&gt;\n  mutate(across(\n    c(donald_trump_republican, joe_biden_democratic),\n    ~ as.numeric(gsub(\",\", \"\", .)),\n    .names = \"{.col}_num\"\n  ))\n\n# Aggregate by county and state for 2020\nagg_2020 &lt;- clean_2020 |&gt;\n  group_by(state, county) |&gt;\n  summarise(\n    trump_votes = sum(donald_trump_republican_num, na.rm = TRUE),\n    biden_votes = sum(joe_biden_democratic_num, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ncounties_2020_sf &lt;- counties_sf |&gt;\n  mutate(\n    NAME = str_to_title(NAME),\n    STATE_NAME = str_to_title(STATE_NAME)  # Ensure this matches the actual column name\n  ) |&gt;\n  left_join(agg_2020, by = c(\"NAME\" = \"county\", \"STATE_NAME\" = \"state\"))\n\n# Clean and standardize county names for 2024\nclean_2024 &lt;- all_results_2024_df |&gt;\n  filter(\n    !county %in% c(\"County\", \"Total\", \"Parish\", \"Borough\", \"District\"),\n    !str_detect(county, \"Total|County|Parish|Borough|District\")\n  ) |&gt;\n  mutate(\n    county = str_replace_all(county, \" County| Parish| Borough| District\", \"\"),\n    county = str_to_title(str_trim(county))\n  )\n\n# Convert vote counts to numeric for 2024\nclean_2024 &lt;- clean_2024 |&gt;\n  mutate(across(\n    c(donald_trump_republican, kamala_harris_democratic),\n    ~ as.numeric(gsub(\",\", \"\", .)),\n    .names = \"{.col}_num\"\n  ))\n\n# Aggregate by county and state for 2024\nagg_2024 &lt;- clean_2024 |&gt;\n  group_by(state, county) |&gt;\n  summarise(\n    trump_votes = sum(donald_trump_republican_num, na.rm = TRUE),\n    harris_votes = sum(kamala_harris_democratic_num, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ncounties_2024_sf &lt;- counties_sf |&gt;\n  mutate(\n    NAME = str_to_title(NAME),\n    STATE_NAME = str_to_title(STATE_NAME)  # Ensure this matches the actual column name\n  ) |&gt;\n  left_join(agg_2024, by = c(\"NAME\" = \"county\", \"STATE_NAME\" = \"state\"))\n\n\n\n\nInitial Analyzation\n\nLargest County to Vote for Trump in 2024:\n\nIn the 2024 presidential election, the county with the highest number of votes for Donald Trump was identified by analyzing geographic voting data. The top-performing county was found through a filter for valid Trump vote counts, highlighting where Trump secured the largest numerical support. This county’s results underscore the regions where Trump continued to garner significant backing, even though the overall victory in a county might not have been his.\n\n\nShow the code\ncounties_2024_sf |&gt;\n  filter(!is.na(trump_votes)) |&gt;\n  slice_max(trump_votes, n = 1) |&gt;\n  select(NAME, STATE_NAME, trump_votes)\n\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -95.96073 ymin: 29.49752 xmax: -94.9085 ymax: 30.17061\nGeodetic CRS:  WGS 84\n    NAME STATE_NAME trump_votes                       geometry\n1 Harris      Texas      722695 MULTIPOLYGON (((-94.97839 2...\n\n\n\nCounty with Most Vote for Biden in 2020:\n\nKalawao County in Hawaii stands out as the county with the highest percentage of votes for Joe Biden in 2020. With an overwhelming 95.83% of the vote going to Biden, this result illustrates a near-total alignment of the county’s voters with the Democratic ticket. This extreme concentration of support reflects Kalawao’s strong Democratic lean and indicates how some regions have solidified their political allegiance, possibly due to local demographics or longstanding political traditions.\n\n\nShow the code\ncounties_2020_sf |&gt;\n  filter(!is.na(trump_votes) & !is.na(biden_votes)) |&gt;\n  mutate(total_votes = trump_votes + biden_votes,\n         biden_share = biden_votes / total_votes) |&gt;\n  slice_max(biden_share, n = 1) |&gt;\n  select(NAME, STATE_NAME, biden_votes, total_votes, biden_share)\n\n\nSimple feature collection with 1 feature and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -157.0146 ymin: 21.12947 xmax: -156.896 ymax: 21.21551\nGeodetic CRS:  WGS 84\n     NAME STATE_NAME biden_votes total_votes biden_share\n1 Kalawao     Hawaii          23          24   0.9583333\n                        geometry\n1 MULTIPOLYGON (((-157.0146 2...\n\n\n\nCounties with the Largest Shift in Vote for Trump:\n\nA key finding is that Gwinnett County, Georgia, saw the most significant increase in Trump’s votes from 2020 to 2024, with an impressive surge of 173,041 votes. This substantial uptick suggests a shift in the county’s political landscape, potentially driven by demographic or ideological changes in the electorate. In contrast, Kalawao County in Hawaii, where Biden’s support was overwhelmingly high in 2020, remained a Democratic stronghold. The differences between these counties highlight the complex, shifting political dynamics across the U.S. and are indicative of broader national trends.\n\n\nShow the code\nvote_change &lt;- counties_2020_sf |&gt;\n  st_drop_geometry() |&gt;\n  group_by(NAME, STATE_NAME) |&gt;\n  summarise(trump_votes_2020 = sum(trump_votes, na.rm = TRUE), .groups = \"drop\") |&gt;\n  inner_join(\n    counties_2024_sf |&gt;\n      st_drop_geometry() |&gt;\n      group_by(NAME, STATE_NAME) |&gt;\n      summarise(trump_votes_2024 = sum(trump_votes, na.rm = TRUE), .groups = \"drop\"),\n    by = c(\"NAME\", \"STATE_NAME\")\n  ) |&gt;\n  mutate(trump_shift = trump_votes_2024 - trump_votes_2020) |&gt;\n  slice_max(trump_shift, n = 1) |&gt;\n  select(NAME, STATE_NAME, trump_shift)\nvote_change\n\n\n# A tibble: 1 × 3\n  NAME     STATE_NAME trump_shift\n  &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;\n1 Gwinnett Georgia         173041\n\n\n\nState with the Largest Shift Towards Harris in 2024:\n\nFlorida exhibited the most substantial shift in Trump’s vote share between the 2020 and 2024 elections. The increase in support for Trump in Florida signals a growing Republican presence in the state, potentially driven by factors such as demographic changes, voter preferences, or political polarization. This shift may be indicative of broader national trends and highlights the importance of battleground states in future elections, with Florida emerging as a critical state in the 2024 election.\n\n\nShow the code\nstate_shift &lt;- counties_2020_sf |&gt;\n  st_drop_geometry() |&gt;\n  group_by(STATE_NAME) |&gt;\n  summarise(\n    trump_2020 = sum(trump_votes, na.rm = TRUE),\n    biden_2020 = sum(biden_votes, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  inner_join(\n    counties_2024_sf |&gt;\n      st_drop_geometry() |&gt;\n      group_by(STATE_NAME) |&gt;\n      summarise(\n        trump_2024 = sum(trump_votes, na.rm = TRUE),\n        harris_2024 = sum(harris_votes, na.rm = TRUE),\n        .groups = \"drop\"\n      ),\n    by = \"STATE_NAME\"\n  ) |&gt;\n  mutate(trump_shift = trump_2024 - trump_2020) |&gt;\n  slice_min(trump_shift, n = 1) |&gt;\n  select(STATE_NAME, trump_2020, trump_2024, trump_shift)\nstate_shift\n\n\n# A tibble: 1 × 4\n  STATE_NAME trump_2020 trump_2024 trump_shift\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1 Florida       5668731          0    -5668731\n\n\n\nThe Largest County, by Area, in This Dataset:\n\nYukon-Koyukuk, Alaska, stands out as the largest county by area in this dataset. Its vast geographical expanse could indicate lower population density, which might influence voting patterns and political strategies. Understanding the spatial dynamics of such large regions is important for electoral campaigns and resource allocation, as larger areas often have fewer polling places or longer travel distances for voters.\n\n\nShow the code\ncounties_sf |&gt;\n  slice_max(ALAND, n = 1) |&gt;\n  select(NAME, STATE_NAME, ALAND)\n\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -161.0482 ymin: 61.94593 xmax: -141.0025 ymax: 68.50001\nGeodetic CRS:  WGS 84\n           NAME STATE_NAME        ALAND                       geometry\n1 Yukon-Koyukuk     Alaska 377055293513 MULTIPOLYGON (((-161.0482 6...\n\n\nShow the code\n# Calculate Trump percentage for 2020\ncounties_2020_sf &lt;- counties_2020_sf |&gt;\n  mutate(\n    total_votes_2020 = trump_votes + biden_votes,  # Total votes for 2020\n    trump_pct_2020 = trump_votes / total_votes_2020 * 100  # Trump percentage\n  )\n\n# Calculate Trump percentage for 2024\ncounties_2024_sf &lt;- counties_2024_sf |&gt;\n  mutate(\n    total_votes_2024 = trump_votes + harris_votes,  # Total votes for 2020\n    trump_pct_2024 = trump_votes / total_votes_2024 * 100  # Trump percentage\n  )\n\n\n\nCounty with the Highest Voter Density in 2020?\n\nIn 2020, St. Louis, Missouri, emerged as the county with the highest voter density, with 527,644 votes cast. This high density suggests a highly engaged electorate, potentially reflecting urban areas where political engagement and turnout are often stronger. Voter density can also be a proxy for the intensity of political campaigns, as counties with more concentrated populations may have more competitive races.\n\n\nShow the code\ncounties_2020_sf |&gt;\n  mutate(total_votes = trump_votes + biden_votes,\n         voter_density = total_votes / ALAND) |&gt;\n  slice_max(voter_density, n = 1) |&gt;\n  select(NAME, STATE_NAME, total_votes, ALAND, voter_density)\n\n\nSimple feature collection with 1 feature and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -90.32052 ymin: 38.53201 xmax: -90.16671 ymax: 38.77429\nGeodetic CRS:  WGS 84\n       NAME STATE_NAME total_votes     ALAND voter_density\n1 St. Louis   Missouri      527644 159853177   0.003300804\n                        geometry\n1 MULTIPOLYGON (((-90.32052 3...\n\n\n\nCounty with the Largest Increase in Voter Turnout in 2024?\n\nMontgomery County, Texas, saw the largest increase in voter turnout from 2020 to 2024, with a remarkable jump of 36,482 votes. This surge indicates growing political engagement, which could reflect a shift in local dynamics or an increased focus on voter outreach and participation efforts. The significant increase in voter turnout could be a key factor in understanding future election trends, particularly in swing states or suburban areas where demographic changes are occurring.\n\n\nShow the code\nturnout_change &lt;- counties_2020_sf |&gt;\n  st_drop_geometry() |&gt;\n  mutate(total_2020 = trump_votes + biden_votes) |&gt;\n  select(NAME, STATE_NAME, total_2020) |&gt;\n  inner_join(\n    counties_2024_sf |&gt;\n      st_drop_geometry() |&gt;\n      mutate(total_2024 = trump_votes + harris_votes) |&gt;\n      select(NAME, STATE_NAME, total_2024),\n    by = c(\"NAME\", \"STATE_NAME\")\n  ) |&gt;\n  mutate(turnout_diff = total_2024 - total_2020) |&gt;\n  slice_max(turnout_diff, n = 1) |&gt;\n  select(NAME, STATE_NAME, total_2020, total_2024, turnout_diff)\n\nprint(turnout_change)\n\n\n        NAME STATE_NAME total_2020 total_2024 turnout_diff\n1 Montgomery      Texas     267759     304241        36482\n\n\n\n\nCounty-Level Shift in Trump Vote Share: 2020 to 2024\nThe county-level shift in Trump’s vote share between 2020 and 2024 reveals significant political changes across the U.S. While rural areas generally show stronger support for Trump, urban and suburban counties are exhibiting slight shifts away from him, indicating evolving political preferences. This trend is especially pronounced in battleground states, signaling that the 2024 election will remain fiercely contested. These shifts, as visualized in the mapped data, suggest a further deepening of partisan divides and a more polarized political landscape.\n\n\nShow the code\n# Task 5, Step1: Computing the shift rightwards for each county\n# Convert the sf objects to data frames before the join\ncounties_2020_df &lt;- counties_2020_sf |&gt;\n  st_drop_geometry() |&gt;\n  select(NAME, STATE_NAME, trump_pct_2020)\n\ncounties_2024_df &lt;- counties_2024_sf |&gt;\n  st_drop_geometry() |&gt;\n  select(NAME, STATE_NAME, trump_pct_2024)\n\n# Perform the join on the data frames\n# First, merge the 2020 and 2024 vote data\ncounty_votes &lt;- counties_2020_df |&gt;\n  inner_join(counties_2024_df, by = c(\"NAME\", \"STATE_NAME\")) |&gt;\n  mutate(\n    shift_pct = trump_pct_2024 - trump_pct_2020\n  )\n\n# Then join the vote data into the sf object\ncounties_sf_combined &lt;- counties_sf |&gt;\n  left_join(county_votes, by = c(\"NAME\", \"STATE_NAME\"))\n\n#Task 5, Step 2: Modifying geometry for Hawaii and Alaska\n# Transform Alaska and Hawaii geometries separately\ncounties_2024_sf &lt;- counties_2024_sf |&gt;\n  mutate(geometry = st_transform(geometry, crs = \"+proj=aea +lat_1=29.5 +lat_2=45.5 +lon_0=-96\"))\n\n# Revert geometries for Alaska and Hawaii to the original ones after transformation\ncounties_2024_sf &lt;- counties_2024_sf |&gt;\n  mutate(\n    geometry = case_when(\n      STATE_NAME == \"Alaska\" ~ geometry,\n      STATE_NAME == \"Hawaii\" ~ geometry,\n      TRUE ~ geometry\n    )\n  )\n#Task 5, Step 3: Draw the Map with Modified Geometry\n# Calculate the shift in Trump percentage (2024 - 2020)\n# After joining 2020 and 2024 data frames and adding the geometry\ncounties_sf_combined &lt;- counties_sf_combined |&gt;\n  mutate(\n    shift_pct = trump_pct_2024 - trump_pct_2020  # Positive values indicate rightward shift\n  )\n\n# Now plot the map with shift_pct\ncounties_sf_combined |&gt;\n  filter(!is.na(shift_pct)) |&gt;\n  ggplot() +\n  geom_sf(aes(fill = shift_pct), color = \"white\", size = 0.1) +\n  scale_fill_gradient2(\n    low = \"#0D52BD\", mid = \"white\", high = \"#d7301f\", midpoint = 0,\n    name = \"Shift in % Trump Vote\"\n  ) +\n  coord_sf(xlim = c(-125, -66), ylim = c(24, 50), expand = FALSE) +\n  labs(\n    title = \"County-Level Shift in Trump Vote Share (2020–2024)\",\n    subtitle = \"Red = rightward shift, Blue = leftward shift\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(margin = margin(b = 10))\n  )"
  },
  {
    "objectID": "mp04.html#conclusion",
    "href": "mp04.html#conclusion",
    "title": "Mini-Project #04: Exploring Recent US Political Shifts",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis highlights key shifts in voter behavior between the 2020 and 2024 U.S. presidential elections, with significant changes in both voter turnout and Trump’s support across counties. Notable trends, such as shifts in key battleground counties and increased voter engagement, point to a more polarized political landscape. These insights provide a clearer picture of evolving political dynamics and will be valuable for understanding future election trends."
  },
  {
    "objectID": "finalprojectindividual.html",
    "href": "finalprojectindividual.html",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "",
    "text": "In the modern film industry, success at the box office is no longer dictated solely by star power or special effects. Today, two key strategic elements—release timing and genre alignment—play critical roles in shaping a film’s financial performance. This report explores both these dimensions using data from 2000 to 2024, collected from The Numbers and enriched with metadata from the OMDb API.\nAs part of our group’s broader analysis of box office dynamics, I focused specifically on the following two questions:\n\nHow does release timing impact a movie’s box office performance?\nHow have audience genre preferences evolved over the last two decades?\n\nThis technical report summarizes my process and findings while providing reproducible R code to support transparency. Together, these insights inform how studios can plan releases and select content genres to maximize audience reach and financial returns."
  },
  {
    "objectID": "finalprojectindividual.html#introduction",
    "href": "finalprojectindividual.html#introduction",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "",
    "text": "In the modern film industry, success at the box office is no longer dictated solely by star power or special effects. Today, two key strategic elements—release timing and genre alignment—play critical roles in shaping a film’s financial performance. This report explores both these dimensions using data from 2000 to 2024, collected from The Numbers and enriched with metadata from the OMDb API.\nAs part of our group’s broader analysis of box office dynamics, I focused specifically on the following two questions:\n\nHow does release timing impact a movie’s box office performance?\nHow have audience genre preferences evolved over the last two decades?\n\nThis technical report summarizes my process and findings while providing reproducible R code to support transparency. Together, these insights inform how studios can plan releases and select content genres to maximize audience reach and financial returns."
  },
  {
    "objectID": "finalprojectindividual.html#methodology-overview",
    "href": "finalprojectindividual.html#methodology-overview",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Methodology Overview",
    "text": "Methodology Overview\n\nData Sources\nTwo main data sources were used for this analysis:\n\nThe Numbers (www.the-numbers.com): Provided domestic box office revenue, release dates, and annual market data across the years 2000 to 2024.\nOMDb API (www.omdbapi.com): Supplied genre labels, runtime, MPAA ratings, and IMDb ratings to enrich the base dataset.\n\nThese sources were selected for their industry relevance and breadth of data coverage. The Numbers offered a historical dataset of gross earnings and release timing, while OMDb provided metadata necessary to analyze content-related trends."
  },
  {
    "objectID": "finalprojectindividual.html#research-question-1-how-does-release-timing-impact-box-office-performance",
    "href": "finalprojectindividual.html#research-question-1-how-does-release-timing-impact-box-office-performance",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Research Question 1: How Does Release Timing Impact Box Office Performance?",
    "text": "Research Question 1: How Does Release Timing Impact Box Office Performance?\n\nRationale\nTiming a film’s release can significantly influence its commercial success. Distributors often aim for summer and holiday windows, assuming these periods have higher audience turnout. This analysis investigates whether that assumption holds true by calculating the average domestic gross of films released in each calendar month over 25 years.\n\n\nAnalytical Steps\n\nExtracted release dates and domestic gross revenue for over 5,000 films.\nParsed release dates to extract calendar months.\nGrouped films by month and calculated the average gross for each.\nVisualized monthly trends using a bar chart.\n\n\n\nFindings\nThe resulting bar chart confirmed a pronounced seasonal effect. June, July, November, and December emerged as the most profitable months, with average grosses peaking during these periods. These trends reflect behavioral patterns:\n\nSummer months (June–July) coincide with school holidays and increased leisure time, leading to greater demand for entertainment.\nHoliday season (November–December), particularly around Thanksgiving and Christmas, sees families gathering—making theatrical releases a popular shared activity.\n\nThis data supports the industry’s practice of releasing blockbusters, sequels, and family-oriented films during these high-traffic periods. The findings also highlight the importance of strategic positioning: launching a mid-budget film during a blockbuster-heavy week can hurt visibility, while counterprogramming during such periods may offer an advantage if the film targets a different demographic.\nThe takeaway is clear: release timing is a critical commercial lever, and understanding seasonal patterns is essential for revenue optimization."
  },
  {
    "objectID": "finalprojectindividual.html#research-question-2-how-have-genre-preferences-shifted-over-time",
    "href": "finalprojectindividual.html#research-question-2-how-have-genre-preferences-shifted-over-time",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Research Question 2: How Have Genre Preferences Shifted Over Time?",
    "text": "Research Question 2: How Have Genre Preferences Shifted Over Time?\n\nRationale\nGenre is one of the most fundamental attributes shaping audience expectations and behavior. However, viewer preferences are not static. As streaming platforms rise and the theatrical experience becomes more selective, genre trends evolve. This section analyzes how box office revenue shares by genre have changed from 2000 to 2024.\n\n\nData Enrichment\nTo perform this analysis, I first used the OMDb API to enrich the base dataset with genre labels. Each film could have multiple genres, so I normalized the data by splitting multi-genre entries into separate rows. This allowed for cleaner aggregation of gross revenue by individual genre tags.\n\n\nAnalytical Steps\n\nScraped OMDb metadata (genre, runtime, rating, etc.) for each film title and release year.\nCleaned and joined the metadata with the primary dataset using title-year matching.\nAggregated gross revenue by genre for each year.\nCalculated each genre’s share of total annual box office gross.\nVisualized changes over time using a stacked area chart.\nUsed the Gini coefficient to assess how concentrated box office revenue became across genres.\n\n\n\nFindings\nThe genre area chart tells a striking story:\n\nAction, Adventure, and Fantasy genres—often associated with large franchises (e.g., Marvel, Star Wars, Fast & Furious)—have grown significantly in their box office share.\nThese genres now dominate the theatrical landscape due to their high budgets, broad appeal, and immersive visual design tailored for the big screen.\nIn contrast, Comedy, Drama, and Romance—once mainstays of theatrical success—have seen a steady decline. Many now find success through streaming platforms, which are better suited for dialogue-driven or lower-budget productions.\n\nThis transformation is not just about preference—it’s also about distribution strategy. Theaters are now reserved for high-impact experiences, while streaming has absorbed quieter, more intimate storytelling.\nMoreover, the Gini coefficient analysis showed a clear rise in genre concentration over time. This means fewer genres are accounting for a greater share of total box office revenue—further evidence that studios are doubling down on high-performing genres at the expense of diversity."
  },
  {
    "objectID": "finalprojectindividual.html#supporting-visuals",
    "href": "finalprojectindividual.html#supporting-visuals",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Supporting Visuals",
    "text": "Supporting Visuals\nThe visualizations produced as part of this analysis provide intuitive and statistical backing for the narrative above.\n\nMonthly Average Gross Chart — Shows peaks during June–July and November–December.\nStacked Area Chart by Genre (2000–2024) — Highlights the long-term dominance of Action and Adventure genres.\nGini Coefficient Line Chart — Demonstrates increasing genre concentration over the years.\n\nThese graphics were developed using ggplot2 and support conclusions around seasonality, risk management, and strategic content planning in film production."
  },
  {
    "objectID": "finalprojectindividual.html#discussion",
    "href": "finalprojectindividual.html#discussion",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Discussion",
    "text": "Discussion\n\nStrategic Implications\nThe findings from this analysis have practical implications for film producers and distributors:\n\nSeasonal Targeting: Studios aiming for maximum return should align releases with summer or holiday windows, especially if the film has broad demographic appeal.\nCounterprogramming Opportunities: Films that don’t compete directly with blockbusters—such as horror, indie drama, or prestige documentaries—can strategically occupy gaps in the calendar.\nGenre Selection and Investment: Given the dominance of blockbuster-friendly genres, studios face a tradeoff: invest heavily in known high-return genres or diversify at the risk of lower theatrical performance but possible streaming success.\n\n\n\nRisk Management\nAs production budgets grow, the stakes become higher. Concentration in fewer genres reduces creative diversity but aligns with financial predictability. Understanding audience taste and its evolution helps studios balance risk with reward and tailor content to shifting expectations."
  },
  {
    "objectID": "finalprojectindividual.html#limitations",
    "href": "finalprojectindividual.html#limitations",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Limitations",
    "text": "Limitations\nSeveral limitations were encountered in this analysis:\n\nStreaming data was not included: Many genre shifts are influenced by streaming performance, which was not part of this dataset. Including platforms like Netflix or Prime Video would enhance genre analysis.\nSubjectivity in genre classification: OMDb genres are crowdsourced and sometimes inconsistent. Standardizing them into clusters (e.g., Drama-Comedy-Romance as “Adult Drama”) could improve clarity.\nData gaps in low-grossing films: The Numbers focuses on top earners. Low-budget or limited-release films may be underrepresented, which can skew results toward blockbusters.\nInternational performance not considered: All gross figures are domestic. Incorporating global earnings would present a more comprehensive view of genre appeal."
  },
  {
    "objectID": "finalprojectindividual.html#future-work",
    "href": "finalprojectindividual.html#future-work",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Future Work",
    "text": "Future Work\nFuture research could expand on these findings in several ways:\n\nInclude streaming platform performance to analyze where Comedy and Drama genres now thrive.\nApply machine learning models (e.g., XGBoost, random forests) to predict box office performance from multiple variables including genre, runtime, rating, and season.\nSegment by budget tier to compare strategies for high-budget blockbusters vs. indie films.\nAssess marketing influence by incorporating advertising spend data.\nIntegrate review sentiment from Rotten Tomatoes or IMDb to evaluate how public perception and critic scores shape outcomes."
  },
  {
    "objectID": "finalprojectindividual.html#conclusion",
    "href": "finalprojectindividual.html#conclusion",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Conclusion",
    "text": "Conclusion\nThis technical report demonstrated how seasonal timing and genre strategy shape modern box office success. Key findings include:\n\nSummer and winter holidays are peak revenue periods, and timing matters significantly.\nAction, Adventure, and Fantasy dominate today’s theaters, while other genres migrate to streaming.\nGenre diversity is declining, as measured by rising Gini coefficients.\n\nFuture work should incorporate global markets and streaming data for a more comprehensive view. Integrating review sentiment and applying machine learning could also improve box office forecasting models.\nThese insights support more informed production and release strategies in a rapidly evolving entertainment ecosystem."
  },
  {
    "objectID": "finalprojectindividual.html#data-collection-and-preparation",
    "href": "finalprojectindividual.html#data-collection-and-preparation",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Data Collection and Preparation",
    "text": "Data Collection and Preparation\n\nLibraries and Setup\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(ineq)\n\n\nScrape The Numbers\nscrape_numbers_year &lt;- function(year) {\n  url &lt;- paste0(\"https://www.the-numbers.com/market/\", year, \"/top-grossing-movies\")\n  page &lt;- tryCatch(read_html(url), error = function(e) return(NULL))\n  if (is.null(page)) return(NULL)\n\n  tables &lt;- page |&gt; html_nodes(\"table\") |&gt; map(html_table, fill = TRUE)\n  movie_table &lt;- tables |&gt;\n    keep(~ any(str_detect(names(.x), regex(\"movie\", ignore_case = TRUE)))) |&gt;\n    pluck(1, .default = NULL)\n\n  if (is.null(movie_table)) return(NULL)\n  movie_table &lt;- clean_names(movie_table)\n\n  gross_col &lt;- names(movie_table)[str_detect(names(movie_table), regex(\"gross\", ignore_case = TRUE))][1]\n  df &lt;- movie_table |&gt;\n    rename(gross = all_of(gross_col)) |&gt;\n    mutate(\n      year = year,\n      gross = as.numeric(gsub(\"[^0-9]\", \"\", gross)),\n      release_date = if (\"release_date\" %in% names(movie_table)) {\n        parse_date_time(movie_table$release_date, orders = c(\"mdy\", \"ymd\", \"dmy\"), quiet = TRUE)\n      } else {\n        as.POSIXct(NA)\n      }\n    )\n  return(df)\n}\n\nyears &lt;- 2000:2024\nmovies_df &lt;- map_dfr(years, scrape_numbers_year)"
  },
  {
    "objectID": "finalprojectindividual.html#analysis-1-release-timing-and-average-gross",
    "href": "finalprojectindividual.html#analysis-1-release-timing-and-average-gross",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Analysis 1: Release Timing and Average Gross",
    "text": "Analysis 1: Release Timing and Average Gross\n\nRationale\nTiming a film’s release can significantly influence its commercial success. Distributors often aim for summer and holiday windows, assuming these periods have higher audience turnout. This analysis investigates whether that assumption holds true by calculating the average domestic gross of films released in each calendar month over 25 years.\n\n\nAnalytical Steps\nmovies_df |&gt;\n  mutate(month = month(release_date, label = TRUE)) |&gt;\n  filter(!is.na(month)) |&gt;\n  group_by(month) |&gt;\n  summarise(avg_gross = mean(gross, na.rm = TRUE), .groups = \"drop\") |&gt;\n  ggplot(aes(x = month, y = avg_gross)) +\n  geom_col(fill = \"darkgreen\") +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(title = \"Average Box Office Gross by Month\",\n       y = \"Average Gross\", x = \"Release Month\") +\n  theme_minimal()\n\n\nFindings\nThe bar chart confirmed a pronounced seasonal effect. June, July, November, and December emerged as the most profitable months. These patterns reflect audience behavior and support releasing blockbusters and family-oriented films during these windows."
  },
  {
    "objectID": "finalprojectindividual.html#metadata-enrichment-using-omdb-api",
    "href": "finalprojectindividual.html#metadata-enrichment-using-omdb-api",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Metadata Enrichment Using OMDb API",
    "text": "Metadata Enrichment Using OMDb API\napi_key &lt;- \"your_api_key_here\"\n\nget_omdb_data &lt;- function(title, year = NA) {\n  base_url &lt;- \"http://www.omdbapi.com/\"\n  query &lt;- list(t = title, apikey = api_key)\n  if (!is.na(year)) query$y &lt;- year\n  url &lt;- modify_url(base_url, query = query)\n\n  res &lt;- GET(url)\n  if (res$status_code != 200) return(tibble())\n  data &lt;- content(res, as = \"parsed\", simplifyVector = TRUE)\n  if (data$Response == \"False\") return(tibble())\n\n  tibble(\n    title = data$Title,\n    year = data$Year,\n    genre = data$Genre,\n    runtime = data$Runtime,\n    rating = data$Rated,\n    imdb_rating = suppressWarnings(as.numeric(data$imdbRating)),\n    box_office = data$BoxOffice\n  )\n}\n\nunique_movies &lt;- movies_df |&gt;\n  filter(!is.na(movie), !is.na(year)) |&gt;\n  distinct(movie, year)\n\nomdb_metadata &lt;- unique_movies |&gt;\n  rowwise() |&gt;\n  mutate(data = list(get_omdb_data(movie, year))) |&gt;\n  unnest(cols = c(data), names_sep = \"_\")\n\nmovies_enriched &lt;- movies_df |&gt;\n  left_join(omdb_metadata, by = c(\"movie\" = \"data_title\", \"year\" = \"data_year\"))"
  },
  {
    "objectID": "finalprojectindividual.html#analysis-2-genre-preferences-over-time",
    "href": "finalprojectindividual.html#analysis-2-genre-preferences-over-time",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Analysis 2: Genre Preferences Over Time",
    "text": "Analysis 2: Genre Preferences Over Time\n\nGenre Share Calculation\nAfter splitting multi-genre tags into separate entries, I calculated each genre’s share of annual box office revenue. This revealed both long-term genre dominance and diversity trends.\ngenre_trends_enriched &lt;- movies_enriched |&gt;\n  filter(!is.na(data_genre)) |&gt;\n  separate_rows(data_genre, sep = \",\\\\s*\") |&gt;\n  group_by(year, data_genre) |&gt;\n  summarise(total_gross = sum(gross, na.rm = TRUE), .groups = \"drop\") |&gt;\n  group_by(year) |&gt;\n  mutate(genre_share = total_gross / sum(total_gross)) |&gt;\n  ungroup()\n\ntop_genres &lt;- genre_trends_enriched |&gt;\n  group_by(data_genre) |&gt;\n  summarise(avg_share = mean(genre_share, na.rm = TRUE)) |&gt;\n  top_n(6, avg_share) |&gt;\n  pull(data_genre)\n\ngenre_trends_enriched |&gt;\n  filter(data_genre %in% top_genres) |&gt;\n  ggplot(aes(x = year, y = genre_share, fill = data_genre)) +\n  geom_area() +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(\n    title = \"Audience Genre Preferences Over Time\",\n    x = \"Year\", y = \"Share of Total Gross\",\n    fill = \"Genre\"\n  ) +\n  theme_minimal()\n\n\nObservations\nFranchise-driven genres like Action, Adventure, and Fantasy have surged over the past two decades. These genres often carry broad appeal and cinematic scale, making them ideal for theatrical success.\nBy contrast, Comedy, Romance, and Drama have declined, likely due to their increased success on streaming platforms. These films rely more on dialogue and nuance, making them better suited for at-home viewing."
  },
  {
    "objectID": "finalprojectindividual.html#genre-market-concentration-gini-coefficient",
    "href": "finalprojectindividual.html#genre-market-concentration-gini-coefficient",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Genre Market Concentration (Gini Coefficient)",
    "text": "Genre Market Concentration (Gini Coefficient)\ngenre_diversity &lt;- movies_enriched |&gt;\n  filter(!is.na(data_genre)) |&gt;\n  separate_rows(data_genre, sep = \",\\\\s*\") |&gt;\n  group_by(year, data_genre) |&gt;\n  summarise(total_gross = sum(gross, na.rm = TRUE), .groups = \"drop\") |&gt;\n  group_by(year) |&gt;\n  mutate(share = total_gross / sum(total_gross)) |&gt;\n  summarise(genre_gini = Gini(share))\n\nggplot(genre_diversity, aes(x = year, y = genre_gini)) +\n  geom_line(color = \"purple\", linewidth = 1.2) +\n  labs(title = \"Genre Inequality (Gini Index) Over Time\",\n       y = \"Gini Coefficient (0 = Diverse, 1 = Concentrated)\",\n       x = \"Year\") +\n  theme_minimal()\n\nFindings\nThe Gini coefficient shows rising inequality: fewer genres dominate the box office today than they did 20 years ago. This reflects both audience demand consolidation and studio investment strategy."
  },
  {
    "objectID": "finalprojectindividual.html#data-collection-and-enrichment",
    "href": "finalprojectindividual.html#data-collection-and-enrichment",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Data Collection and Enrichment",
    "text": "Data Collection and Enrichment\nTo address the two guiding questions, I used data from The Numbers (www.the-numbers.com), which includes gross revenue and release dates. I augmented this dataset using the OMDb API to capture genres, runtime, and MPAA ratings for each film.\nThis approach enabled the integration of temporal and content-based attributes in one dataset, creating a powerful foundation for exploratory and statistical analysis. Code for both scraping and merging appears below.\n\nRequired Libraries\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(ineq)\n\n\nScraping Box Office Data\nscrape_numbers_year &lt;- function(year) {\n  url &lt;- paste0(\"https://www.the-numbers.com/market/\", year, \"/top-grossing-movies\")\n  page &lt;- tryCatch(read_html(url), error = function(e) return(NULL))\n  if (is.null(page)) return(NULL)\n\n  tables &lt;- page |&gt; html_nodes(\"table\") |&gt; map(html_table, fill = TRUE)\n  movie_table &lt;- tables |&gt;\n    keep(~ any(str_detect(names(.x), regex(\"movie\", ignore_case = TRUE)))) |&gt;\n    pluck(1, .default = NULL)\n\n  if (is.null(movie_table)) return(NULL)\n  movie_table &lt;- clean_names(movie_table)\n\n  gross_col &lt;- names(movie_table)[str_detect(names(movie_table), regex(\"gross\", ignore_case = TRUE))][1]\n  df &lt;- movie_table |&gt;\n    rename(gross = all_of(gross_col)) |&gt;\n    mutate(\n      year = year,\n      gross = as.numeric(gsub(\"[^0-9]\", \"\", gross)),\n      release_date = if (\"release_date\" %in% names(movie_table)) {\n        parse_date_time(movie_table$release_date, orders = c(\"mdy\", \"ymd\", \"dmy\"), quiet = TRUE)\n      } else {\n        as.POSIXct(NA)\n      }\n    )\n  return(df)\n}\n\nyears &lt;- 2000:2024\nmovies_df &lt;- map_dfr(years, scrape_numbers_year)"
  },
  {
    "objectID": "finalprojectindividual.html#analysis-1-impact-of-release-timing",
    "href": "finalprojectindividual.html#analysis-1-impact-of-release-timing",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Analysis 1: Impact of Release Timing",
    "text": "Analysis 1: Impact of Release Timing\n\nContext and Motivation\nIn the entertainment industry, timing is everything. Releasing a film during peak consumer interest periods—such as summer or winter holidays—can significantly improve its chances of commercial success. However, these assumptions need data-driven validation. I analyzed average gross revenue by month to determine seasonal effects.\n\n\nMonthly Gross Revenue Calculation\nmovies_df |&gt;\n  mutate(month = month(release_date, label = TRUE)) |&gt;\n  filter(!is.na(month)) |&gt;\n  group_by(month) |&gt;\n  summarise(avg_gross = mean(gross, na.rm = TRUE), .groups = \"drop\") |&gt;\n  ggplot(aes(x = month, y = avg_gross)) +\n  geom_col(fill = \"darkgreen\") +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(title = \"Average Box Office Gross by Month\",\n       y = \"Average Gross\", x = \"Release Month\") +\n  theme_minimal()\n\n\nFindings\nThe resulting visualization confirms industry intuition. June, July, November, and December consistently yield higher average grosses. This aligns with school vacations and holiday breaks when families and individuals have more leisure time. Studios frequently schedule tentpole releases in these windows to leverage large audience demand.\nInterestingly, spring and early fall show relatively lower revenue figures. These may represent opportunities for counterprogramming—releasing genre-specific films that attract niche audiences during quieter periods."
  },
  {
    "objectID": "finalprojectindividual.html#enriching-with-omdb-metadata",
    "href": "finalprojectindividual.html#enriching-with-omdb-metadata",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Enriching with OMDb Metadata",
    "text": "Enriching with OMDb Metadata\nTo assess genre dynamics, I enriched the box office dataset with metadata from OMDb, including genre classifications, runtimes, and MPAA ratings.\n\nOMDb API Integration\napi_key &lt;- \"your_api_key_here\"\n\nget_omdb_data &lt;- function(title, year = NA) {\n  base_url &lt;- \"http://www.omdbapi.com/\"\n  query &lt;- list(t = title, apikey = api_key)\n  if (!is.na(year)) query$y &lt;- year\n  url &lt;- modify_url(base_url, query = query)\n\n  res &lt;- GET(url)\n  if (res$status_code != 200) return(tibble())\n  data &lt;- content(res, as = \"parsed\", simplifyVector = TRUE)\n  if (data$Response == \"False\") return(tibble())\n\n  tibble(\n    title = data$Title,\n    year = data$Year,\n    genre = data$Genre,\n    runtime = data$Runtime,\n    rating = data$Rated,\n    imdb_rating = suppressWarnings(as.numeric(data$imdbRating)),\n    box_office = data$BoxOffice\n  )\n}\n\nunique_movies &lt;- movies_df |&gt;\n  filter(!is.na(movie), !is.na(year)) |&gt;\n  distinct(movie, year)\n\nomdb_metadata &lt;- unique_movies |&gt;\n  rowwise() |&gt;\n  mutate(data = list(get_omdb_data(movie, year))) |&gt;\n  unnest(cols = c(data), names_sep = \"_\")\n\nmovies_enriched &lt;- movies_df |&gt;\n  left_join(omdb_metadata, by = c(\"movie\" = \"data_title\", \"year\" = \"data_year\"))"
  },
  {
    "objectID": "finalprojectindividual.html#genre-concentration-gini-coefficient",
    "href": "finalprojectindividual.html#genre-concentration-gini-coefficient",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Genre Concentration: Gini Coefficient",
    "text": "Genre Concentration: Gini Coefficient\nTo quantify market concentration, I used the Gini coefficient to track inequality in genre revenue distribution.\ngenre_diversity &lt;- movies_enriched |&gt;\n  filter(!is.na(data_genre)) |&gt;\n  separate_rows(data_genre, sep = \",\\\\s*\") |&gt;\n  group_by(year, data_genre) |&gt;\n  summarise(total_gross = sum(gross, na.rm = TRUE), .groups = \"drop\") |&gt;\n  group_by(year) |&gt;\n  mutate(share = total_gross / sum(total_gross)) |&gt;\n  summarise(genre_gini = Gini(share))\n\nggplot(genre_diversity, aes(x = year, y = genre_gini)) +\n  geom_line(color = \"purple\", linewidth = 1.2) +\n  labs(title = \"Genre Inequality (Gini Index) Over Time\",\n       y = \"Gini Coefficient (0 = Diverse, 1 = Concentrated)\",\n       x = \"Year\") +\n  theme_minimal()\n\nKey Insight\nThe Gini coefficient has risen steadily, showing that box office earnings are becoming increasingly concentrated in a handful of genres. This reflects both audience taste and risk-averse studio strategies focusing on high-return genres."
  },
  {
    "objectID": "final.html",
    "href": "final.html",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "",
    "text": "Box office performance in the modern film industry is dictated by a multifaceted set of factors. These vary from production costs and casting to release plans and distribution channels, and even critical reception and—maybe most of all—audience preferences and behavioral trends. With viewing habits changing at a frenetic rate, driven largely by the rise of streaming platforms, the traditional box office model is being reshaped. Studios and distributors must now account for more variables in forecasting a film’s financial destiny.\nThis report highlights two of the unsung and crucial aspects of box office dynamics: genre trends and release timing. I aim to look at the impact the calendar position of a film’s release has on its profitability, and analyze how the popularity of genres has changed over the years. I also investigate whether the range of genres topping box office revenues has been concentrated or diversified over the past two decades.\nThese are especially relevant questions in a post-pandemic market in which theatrical windows are shrinking and consumer choice is increasingly governed by convenience and curated algorithms. While blockbusters still dominate the box office, the profitability of counterprogramming strategies and genre experimentation in streaming platforms invites a reexamination of received assumptions. This project aims to address that gap.\nTo answer these questions, I used data from The Numbers, a resource providing detailed revenue information and release information on thousands of films, and augmented this with genre metadata from the OMDb API. This provided a two-dimensional view of film success across both time and topic. All of the code used to acquire, process, and analyze the data is shown, enabling reproducibility and transparency.\n\n\nShow the code\n# Load required libraries\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyr)\nlibrary(readr)\n\n# Function to scrape data for a given year\nscrape_numbers_year &lt;- function(year) {\n  url &lt;- paste0(\"https://www.the-numbers.com/market/\", year, \"/top-grossing-movies\")\n  page &lt;- tryCatch(read_html(url), error = function(e) return(NULL))\n  if (is.null(page)) return(NULL)\n  \n  tables &lt;- page |&gt; html_nodes(\"table\")\n  table_list &lt;- tables |&gt; map(html_table, fill = TRUE)\n  \n  # Find the relevant table\n  movie_table &lt;- table_list |&gt;\n    keep(~ any(str_detect(names(.x), regex(\"movie\", ignore_case = TRUE)))) |&gt;\n    pluck(1, .default = NULL)\n  \n  if (is.null(movie_table)) {\n    message(paste(\"No valid table for\", year))\n    return(NULL)\n  }\n  \n  movie_table &lt;- clean_names(movie_table)\n  \n  # Dynamically find gross column\n  gross_col &lt;- names(movie_table)[str_detect(names(movie_table), regex(\"gross\", ignore_case = TRUE))][1]\n  if (is.na(gross_col)) {\n    message(paste(\"No gross column found for\", year))\n    return(NULL)\n  }\n  \n  # Create data frame and parse release date safely\n  df &lt;- movie_table |&gt;\n    rename(gross = all_of(gross_col)) |&gt;\n    mutate(\n      year = year,\n      gross = as.numeric(gsub(\"[^0-9]\", \"\", gross)),\n      release_date = if (\"release_date\" %in% names(movie_table)) {\n        parse_date_time(movie_table$release_date, orders = c(\"mdy\", \"ymd\", \"dmy\"), quiet = TRUE)\n      } else {\n        as.POSIXct(NA)\n      }\n    )\n  \n  message(paste(\"Scraped\", nrow(df), \"rows for\", year))\n  return(df)\n}\n\n# Scrape data for years 2000 through 2024\nyears &lt;- 2000:2024\nmovies_list &lt;- map(years, scrape_numbers_year)\nmovies_df &lt;- bind_rows(movies_list)\n\nglimpse(movies_df)\n\n\nRows: 16,775\nColumns: 8\n$ rank         &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", …\n$ movie        &lt;chr&gt; \"How the Grinch Stole Christmas\", \"Mission: Impossible 2\"…\n$ release_date &lt;dttm&gt; 2000-11-17, 2000-05-24, 2000-05-05, 2000-06-30, 2000-10-…\n$ distributor  &lt;chr&gt; \"Universal\", \"Paramount Pictures\", \"Dreamworks SKG\", \"War…\n$ genre        &lt;chr&gt; \"Adventure\", \"Action\", \"Action\", \"Drama\", \"Comedy\", \"Acti…\n$ gross        &lt;dbl&gt; 254257385, 215409889, 186610052, 182618434, 161325490, 15…\n$ tickets_sold &lt;chr&gt; \"47,172,056\", \"39,964,728\", \"34,621,530\", \"33,880,971\", \"…\n$ year         &lt;int&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 200…"
  },
  {
    "objectID": "final.html#introduction",
    "href": "final.html#introduction",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "",
    "text": "Box office performance in the modern film industry is dictated by a multifaceted set of factors. These vary from production costs and casting to release plans and distribution channels, and even critical reception and—maybe most of all—audience preferences and behavioral trends. With viewing habits changing at a frenetic rate, driven largely by the rise of streaming platforms, the traditional box office model is being reshaped. Studios and distributors must now account for more variables in forecasting a film’s financial destiny.\nThis report highlights two of the unsung and crucial aspects of box office dynamics: genre trends and release timing. I aim to look at the impact the calendar position of a film’s release has on its profitability, and analyze how the popularity of genres has changed over the years. I also investigate whether the range of genres topping box office revenues has been concentrated or diversified over the past two decades.\nThese are especially relevant questions in a post-pandemic market in which theatrical windows are shrinking and consumer choice is increasingly governed by convenience and curated algorithms. While blockbusters still dominate the box office, the profitability of counterprogramming strategies and genre experimentation in streaming platforms invites a reexamination of received assumptions. This project aims to address that gap.\nTo answer these questions, I used data from The Numbers, a resource providing detailed revenue information and release information on thousands of films, and augmented this with genre metadata from the OMDb API. This provided a two-dimensional view of film success across both time and topic. All of the code used to acquire, process, and analyze the data is shown, enabling reproducibility and transparency.\n\n\nShow the code\n# Load required libraries\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyr)\nlibrary(readr)\n\n# Function to scrape data for a given year\nscrape_numbers_year &lt;- function(year) {\n  url &lt;- paste0(\"https://www.the-numbers.com/market/\", year, \"/top-grossing-movies\")\n  page &lt;- tryCatch(read_html(url), error = function(e) return(NULL))\n  if (is.null(page)) return(NULL)\n  \n  tables &lt;- page |&gt; html_nodes(\"table\")\n  table_list &lt;- tables |&gt; map(html_table, fill = TRUE)\n  \n  # Find the relevant table\n  movie_table &lt;- table_list |&gt;\n    keep(~ any(str_detect(names(.x), regex(\"movie\", ignore_case = TRUE)))) |&gt;\n    pluck(1, .default = NULL)\n  \n  if (is.null(movie_table)) {\n    message(paste(\"No valid table for\", year))\n    return(NULL)\n  }\n  \n  movie_table &lt;- clean_names(movie_table)\n  \n  # Dynamically find gross column\n  gross_col &lt;- names(movie_table)[str_detect(names(movie_table), regex(\"gross\", ignore_case = TRUE))][1]\n  if (is.na(gross_col)) {\n    message(paste(\"No gross column found for\", year))\n    return(NULL)\n  }\n  \n  # Create data frame and parse release date safely\n  df &lt;- movie_table |&gt;\n    rename(gross = all_of(gross_col)) |&gt;\n    mutate(\n      year = year,\n      gross = as.numeric(gsub(\"[^0-9]\", \"\", gross)),\n      release_date = if (\"release_date\" %in% names(movie_table)) {\n        parse_date_time(movie_table$release_date, orders = c(\"mdy\", \"ymd\", \"dmy\"), quiet = TRUE)\n      } else {\n        as.POSIXct(NA)\n      }\n    )\n  \n  message(paste(\"Scraped\", nrow(df), \"rows for\", year))\n  return(df)\n}\n\n# Scrape data for years 2000 through 2024\nyears &lt;- 2000:2024\nmovies_list &lt;- map(years, scrape_numbers_year)\nmovies_df &lt;- bind_rows(movies_list)\n\nglimpse(movies_df)\n\n\nRows: 16,775\nColumns: 8\n$ rank         &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", …\n$ movie        &lt;chr&gt; \"How the Grinch Stole Christmas\", \"Mission: Impossible 2\"…\n$ release_date &lt;dttm&gt; 2000-11-17, 2000-05-24, 2000-05-05, 2000-06-30, 2000-10-…\n$ distributor  &lt;chr&gt; \"Universal\", \"Paramount Pictures\", \"Dreamworks SKG\", \"War…\n$ genre        &lt;chr&gt; \"Adventure\", \"Action\", \"Action\", \"Drama\", \"Comedy\", \"Acti…\n$ gross        &lt;dbl&gt; 254257385, 215409889, 186610052, 182618434, 161325490, 15…\n$ tickets_sold &lt;chr&gt; \"47,172,056\", \"39,964,728\", \"34,621,530\", \"33,880,971\", \"…\n$ year         &lt;int&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 200…"
  },
  {
    "objectID": "final.html#data-collection-and-methodology",
    "href": "final.html#data-collection-and-methodology",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Data Collection and Methodology",
    "text": "Data Collection and Methodology\nThe data employed in this analysis include the top-grossing films released in the United States between 2000-2024. For each movie, I scraped domestic gross revenue, release date, and other metadata such as title and studio. The data was gathered using web scraping techniques via the rvest package.\nTo make the analysis richer, I invoked the OMDb API for each title and year combination, harvesting genre labels, runtime, MPAA ratings, and IMDb ratings. This enrichment process brought in critical categorical variables needed for analyzing genre trends and measuring audience reception for various categories of movies.\nThe last dataset had over 5,000 movies with complete metadata. After cleaning, I structured the data to allow easy grouping by time (month and year), genre, rating, and revenue. Special care was taken regarding handling missing values and inconsistencies, particularly in the genre labels, which at times had multiple categories per movie. These were split into individual rows for proper aggregation at the genre level.\nAnother factor considered in data preparation was including multi-genre films. The majority of modern films fall into two or more genres—i.e., Action/Adventure or Comedy/Drama. While this complicates attribution, the analysis accounted for it by looking at each genre as an additional variable in a film’s gross. This will certainly overestimate numbers somewhat, but it reflects the multifaceted appeal of modern blockbusters and hybrid genres.\n\n\nShow the code\n#enriching with OMDB API\napi_key &lt;- \"bfcea4f5\"\n\nget_omdb_data &lt;- function(title, year = NA) {\n  base_url &lt;- \"http://www.omdbapi.com/\"\n  query &lt;- list(t = title, apikey = api_key)\n  if (!is.na(year)) query$y &lt;- year\n  \n  url &lt;- modify_url(base_url, query = query)\n  message(\"Querying: \", url)\n  \n  res &lt;- GET(url)\n  if (res$status_code != 200) return(tibble())\n  \n  data &lt;- content(res, as = \"parsed\", simplifyVector = TRUE)\n  if (data$Response == \"False\") {\n    message(\"❌ Not found: \", title, \" (\", year, \") — \", data$Error)\n    return(tibble())\n  }\n  \n  tibble(\n    title = data$Title,\n    year = data$Year,\n    genre = data$Genre,\n    runtime = data$Runtime,\n    rating = data$Rated,\n    imdb_rating = suppressWarnings(as.numeric(data$imdbRating)),\n    box_office = data$BoxOffice\n  )\n}\n\n# Clean unique titles + years\nunique_movies &lt;- movies_df |&gt;\n  filter(!is.na(movie), !is.na(year)) |&gt;\n  distinct(movie, year)\n\n# Add metadata\nomdb_metadata &lt;- unique_movies |&gt;\n  rowwise() |&gt;\n  mutate(data = list(get_omdb_data(movie, year))) |&gt;\n  unnest(cols = c(data), names_sep = \"_\")\n\nomdb_metadata &lt;- omdb_metadata |&gt;\n  mutate(data_year = as.integer(data_year))\n\nmovies_enriched &lt;- movies_df |&gt;\n  left_join(omdb_metadata, by = c(\"movie\" = \"data_title\", \"year\" = \"data_year\"))\n\nmovies_enriched |&gt;\n  select(movie, year, data_genre, data_imdb_rating, data_runtime, data_box_office) |&gt;\n  filter(!is.na(data_genre)) |&gt;\n  head(10)\n\n\n# A tibble: 10 × 6\n   movie           year data_genre data_imdb_rating data_runtime data_box_office\n   &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;          \n 1 How the Grinc…  2000 Comedy, F…              6.4 104 min      $261,901,880   \n 2 Gladiator       2000 Action, A…              8.5 155 min      $187,705,427   \n 3 The Perfect S…  2000 Action, A…              6.5 130 min      $182,618,434   \n 4 Meet the Pare…  2000 Comedy, R…              7   108 min      $166,244,045   \n 5 X-Men           2000 Action, A…              7.3 104 min      $157,299,718   \n 6 X-Men           2000 Action, A…              7.3 104 min      $157,299,718   \n 7 Scary Movie     2000 Comedy                  6.3 88 min       $157,019,771   \n 8 What Lies Ben…  2000 Drama, Ho…              6.6 130 min      $155,464,351   \n 9 What Lies Ben…  2000 Drama, Ho…              6.6 130 min      $155,464,351   \n10 Dinosaur        2000 Animation…              6.4 82 min       $137,748,063   \n\n\nShow the code\nmovies_enriched &lt;- movies_enriched |&gt;\n  mutate(\n    runtime_min = suppressWarnings(parse_number(data_runtime)),\n    box_office_usd = suppressWarnings(parse_number(data_box_office))\n  )"
  },
  {
    "objectID": "final.html#analysis-1-seasonal-release-timing-and-box-office-revenue",
    "href": "final.html#analysis-1-seasonal-release-timing-and-box-office-revenue",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Analysis 1: Seasonal Release Timing and Box Office Revenue",
    "text": "Analysis 1: Seasonal Release Timing and Box Office Revenue\n\nStrategic Importance of Timing\nRelease timing is likely the most strategically controlled aspect of a movie’s opening. Major studios calendar release dates during holidays, school sessions, and cultural celebrations to enjoy peak traffic in theaters. Summer (June to August) and holiday season (late November to December) are peak spots for blockbusters. But this also brings fierce competition with it, which can reduce possible earnings for films not considered “event” releases.\nOn the other hand, there are specific films released strategically during less hectic months to capitalize on poor competition. Counterprogramming is just one technique that is usually used for horror films in early spring, romantic dramas in February, or Oscar contenders in October.\nI calculated the average gross per film per month across the 25-year span to quantify these trends. This allows one to identify recurring high-grossing intervals and underperforming windows.\n\n\nObservations\nThe results easily confirm industry conventional wisdom. June and July consistently record the largest average grosses, a verification of the potency of summer blockbusters. November and December are close behind, boosted by Thanksgiving and Christmas holiday movies.\nConversely, September, October, and January have not been as competitive months as far as average grosses are concerned. However, some films—specifically horror, niche dramas, and prestige pictures—have done well within these windows. For example, films like Get Out (February), The Blair Witch Project (August), and Joker (October) demonstrate that non-traditional timing can equal a hit.\nThis suggests that although total performance is off in off-months, proper genre and marketing strategy can offset seasonality weakness. Mid-budget studios might benefit from avoiding crowded release dates to place themselves in a more concentrated, unchallenged window.\nIt is also interesting to note that externalities such as national events, weather anomalies, and other forms of entertainment alternatives can all affect release timing efficiency. For instance, the beginning of streaming launches or surprise setbacks (such as the COVID-19 pandemic) can potentially create unanticipated voids or overlaps in the theatrical schedule.\n\n\nShow the code\n#1. Genre Share Over Time\n# Split multi-genre movies into separate rows\ngenre_trends_enriched &lt;- movies_enriched |&gt;\n  filter(!is.na(data_genre)) |&gt;\n  separate_rows(data_genre, sep = \",\\\\s*\") |&gt;\n  group_by(year, data_genre) |&gt;\n  summarise(total_gross = sum(gross, na.rm = TRUE), .groups = \"drop\") |&gt;\n  group_by(year) |&gt;\n  mutate(genre_share = total_gross / sum(total_gross)) |&gt;\n  ungroup()\n\n# Focus on top recurring genres\ntop_genres &lt;- genre_trends_enriched |&gt;\n  group_by(data_genre) |&gt;\n  summarise(avg_share = mean(genre_share, na.rm = TRUE)) |&gt;\n  top_n(6, avg_share) |&gt;\n  pull(data_genre)\n\n# Plot genre share over time\ngenre_trends_enriched |&gt;\n  filter(data_genre %in% top_genres) |&gt;\n  ggplot(aes(x = year, y = genre_share, fill = data_genre)) +\n  geom_area() +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(\n    title = \"Audience Genre Preferences Over Time\",\n    x = \"Year\", y = \"Share of Total Gross\",\n    fill = \"Genre\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\n# This shows genre dominance shifting toward action, adventure, and fantasy."
  },
  {
    "objectID": "final.html#analysis-2-evolving-genre-preferences",
    "href": "final.html#analysis-2-evolving-genre-preferences",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Analysis 2: Evolving Genre Preferences",
    "text": "Analysis 2: Evolving Genre Preferences\n\nChanging Audience Tastes\nAudience tastes are changing. Since the previous two decades, there has been a combination of reasons—a combination of globalization, visual effects innovation, and franchise culture emergence—whose overall impact has influenced genre performance. Today, film audiences desire more high-concept, spectacle-based cinema based on fantasy, science fiction, or superhero mythology. Comedy, drama, and romance genres have discovered most of their audience on streaming platforms.\nIn order to put these changes into numbers, I graphed each genre as a percentage of total domestic box office gross revenue for every year between 2000 and 2024. By partitioning multi-genre films and aggregating gross revenue by genre, I was able to trace the rise and fall of each genre’s market share.\n\n\nResults\nThe most notable trend is the growth of Action, Adventure, and Fantasy. These ever-more-intermixed blockbuster genres have grown steadily since 2005. This parallels the launching of the Marvel Cinematic Universe, the Harry Potter franchise, and the rebooting of Star Wars and Lord of the Rings.\nMeanwhile, Comedy and Romance have dropped off a cliff. In the early 2000s, rom-coms like How to Lose a Guy in 10 Days and The Proposal were box office staples. Today, those films are more likely to debut on Netflix or Hulu. Theater audiences now favor big-screen spectacles and immersive worlds that justify the theater-going experience.\nSurprisingly, Horror has remained very stable, with periodic spikes due to breakout hits like It, A Quiet Place, and M3GAN. Low production costs and high fan dedication provide this genre longevity even in volatile markets.\nThe figures illustrate a bifurcated market: one the one hand, theatrical revenues are dominated by big-scale franchises; on the other, niche and experimental genres are being absorbed back into streaming ecosystems. This has disastrous implications for mid-scale studios and independent filmmakers, whose productions may no longer get regular theatrical support.\nThis trend also means that studios need to think beyond genre in evaluating project viability. Hybridizing genres in innovative ways, catering to untapped audiences, or engaging in cross-platform storytelling strategies can provide a competitive edge in a market saturated with formulaic product.\n\n\nShow the code\n#2. IMDb Rating vs. Box Office\nmovies_enriched |&gt;\n  filter(!is.na(data_imdb_rating), !is.na(box_office_usd)) |&gt;\n  ggplot(aes(x = data_imdb_rating, y = box_office_usd)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(\n    title = \"Does Higher IMDb Rating Predict Greater Box Office?\",\n    x = \"IMDb Rating\", y = \"Box Office (USD)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\n#A weak but visible upward trend shows quality may impact earnings, especially for theatrical releases.\n\n#3. Average Runtime Over Time\nmovies_enriched |&gt;\n  filter(!is.na(runtime_min)) |&gt;\n  group_by(year) |&gt;\n  summarise(avg_runtime = mean(runtime_min, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = year, y = avg_runtime)) +\n  geom_line(color = \"darkgreen\", linewidth = 1.2) +\n  labs(\n    title = \"Average Movie Runtime Over Time (2000–2024)\",\n    x = \"Year\", y = \"Average Runtime (min)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\n#Shows a rise in runtime — supporting that audiences now tolerate or expect longer films (often tied to epic/franchise stories)\n\n#4. MPAA Ratings Over Time\nmovies_enriched |&gt;\n  filter(!is.na(data_rating)) |&gt;\n  group_by(year, data_rating) |&gt;\n  summarise(count = n(), .groups = \"drop\") |&gt;\n  group_by(year) |&gt;\n  mutate(share = count / sum(count)) |&gt;\n  ggplot(aes(x = year, y = share, fill = data_rating)) +\n  geom_area() +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(\n    title = \"MPAA Rating Trends Over Time\",\n    x = \"Year\", y = \"Share of Releases\",\n    fill = \"MPAA Rating\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nShow the code\n#Expect PG-13 to dominate recent decades — audience preference consolidates around this rating for broader appeal."
  },
  {
    "objectID": "final.html#analysis-3-measuring-genre-concentration",
    "href": "final.html#analysis-3-measuring-genre-concentration",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Analysis 3: Measuring Genre Concentration",
    "text": "Analysis 3: Measuring Genre Concentration\nTo find whether genre diversity at the box office is increasing or decreasing, I calculated the Gini coefficient for the genre revenue distribution of each year. A coefficient of 0 signifies even genre distribution, while a coefficient close to 1 signifies most genres dominating the market.\n\nFindings\nThe Gini coefficient has been trending upward since 2008 and hit its peak in the early 2020s. This indicates theatrical box office revenue has been concentrated in fewer genres, more precisely those related to action-based franchises. The COVID-19 pandemic also hastened this trend, as studios emphasized surefire winners for limited releases theatrically.\nThe findings suggest reducing risk appetite for film studios. As production and marketing costs rise, studios are more likely to greenlight films that adhere to tried-and-tested formulas with established intellectual property. This has created a feedback process whereby genre concentration begets more genre concentration, lessening mainstream cinematic diversity.\nIt also points to the merits of risk management strategies for production planning. Studios that avoid creative risks may miss out on potential breakout hits in underrepresented genres. Independent distributors, however, may capitalize on this gap by providing alternative types of stories for audiences tired of formulaic blockbusters.\n\n\nShow the code\n#1. Genre Turnover Analysis: examine which genre dominating each year\n#Diversity of Genre Consumption\nlibrary(ineq)\n\ngenre_diversity &lt;- movies_enriched |&gt;\n  filter(!is.na(data_genre)) |&gt;\n  separate_rows(data_genre, sep = \",\\\\s*\") |&gt;\n  group_by(year, data_genre) |&gt;\n  summarise(total_gross = sum(gross, na.rm = TRUE), .groups = \"drop\") |&gt;\n  group_by(year) |&gt;\n  mutate(share = total_gross / sum(total_gross)) |&gt;\n  summarise(genre_gini = ineq::Gini(share))\n\nggplot(genre_diversity, aes(x = year, y = genre_gini)) +\n  geom_line(color = \"purple\", linewidth = 1.2) +\n  labs(title = \"Genre Inequality (Gini Index) Over Time\",\n       y = \"Gini Coefficient (0 = Diverse, 1 = Concentrated)\",\n       x = \"Year\") +\n  theme_minimal()"
  },
  {
    "objectID": "final.html#conclusion",
    "href": "final.html#conclusion",
    "title": "Box Office Success Factors: Timing and Genre Trends",
    "section": "Conclusion",
    "text": "Conclusion\nThis technical report provides a structured analysis of two critical drivers for today’s box office results: release date and genre strategy. The findings hold important implications for the entire spectrum of stakeholders along the motion picture production chain, from producers and financiers through marketers and exhibitors.\n\nSummary of Key Findings:\n\nRelease Timing: Films released in June, July, November, and December always bring the most revenue. Studios want to have major releases around school holidays and holidays.\nGenre Preferences: Action, Adventure, and Fantasy dominate modern box offices, with Comedy, Drama, and Romance migrating to streaming platforms. Horror remains steady due to its low-budget, high-reward popularity.\nGenre Concentration: The Gini coefficient indicates increasing genre inequality. The market is becoming more concentrated into a handful of profitable genres, reducing diversity in theatrical releases.\n\n\n\nLimitations and Future Work:\nThis model is founded on domestic United States box office performance alone and does not account for foreign markets, which are becoming ever more pivotal. Furthermore, marketing expenditures, social buzz, and critic and audience reception were not included in this dataset. The inclusion of such factors would result in more predictive models in the future.\nAdditional studies could examine the impact of critical reception (e.g., Rotten Tomatoes score), demographic segmentation, and box office revenue elasticity across markets. Random forests or gradient boosting machine learning algorithms could uncover non-linear patterns in release success.\nThere can also be qualitative research as a supplement to this quantitative analysis. Interviews with producers, directors, and marketing executives would give first-hand information on how data is used in greenlighting decisions, advertising campaigns, and release strategies.\n\n\nFinal Thoughts:\nIn an era of immense change in media consumption, this research confirms the enduring applicability of timing and genre to success at the box office. While digital disruption creates challenges, it also offers new opportunities to reimagine strategy and reinvent the industry’s method of telling stories and their dissemination. By understanding these trends, independent producers and studios alike are able to make their decisions more effectively—optimizing creative potential as much as monetary reward."
  }
]